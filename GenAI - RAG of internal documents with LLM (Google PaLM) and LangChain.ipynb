{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "359697d5"
   },
   "source": [
    "# GenAI - RAG of internal documents with LLM (Google PaLM) and LangChain, for a Question/Answer system (e.g. \"Fact Pack\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-PE3KRj4zpS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "With plenty of OpenAI's implementations (e.g. text-embedding-3-small), this one implements Google PaLM's text-bison@002 with LangChain. \n",
    "\n",
    "The example is to build a Question Answering system, with prompt engineering (of course) + augmenting the LLM's knowledge with data sources such as documents curated internally. \n",
    "\n",
    "Specifically, the Retrieval Augmented Generation (RAG) is based on a private collection of documents (sample of Google published research papers stored on gs://github-repo/documents/google-research-pdfs/). \n",
    "\n",
    "Furthermore, it prevents hallucination in two ways:\n",
    "1. Provide answers with references to the sources \n",
    "2. Say \"I don't know\" rather than hallucination, such as in the case of being asked \"what is NFC\" where no information is provided in the documents, the system shall not hallucinate. \n",
    "\n",
    "## Steps\n",
    "Ingest knowledge-base sources into a vector store:\n",
    "1. Read the documents, in this case, PDF files\n",
    "2. Chunk the documents, to include relevant parts of the document as context to the prompt\n",
    "3. Generate embeddings for each chunked document\n",
    "4. Add embedding to the vector store\n",
    "\n",
    "When user enters a prompt:\n",
    "1. Raise the question \n",
    "2. Generate embedding for the user prompt \n",
    "3. Search the vector store to retrieve the nearest embeddings (relevant documents) closer to the prompt\n",
    "4. Fetch the actual text for the retrieved embeddings to add as context to the user's prompt\n",
    "5. Add the retrieved documents as context to the user's prompt\n",
    "6. Send the updated prompt to the LLM\n",
    "7. Return a summarized response to the user with references to the sources from the knowledge base\n",
    "\n",
    "## Implementation \n",
    "1. Google's PaLM API through Vertex AI with LangChain RetrievalQA \n",
    "2. Extract text from PDF files, as stored on Cloud Storage bucket\n",
    "3. Generate embeddings \n",
    "4. Vector Search (previously Vertex AI Matching Engine - and, hehe, Google hasn't completed re-branding)\n",
    "5. Query and return relevant results\n",
    "6. Google PaLM API for Text as LLM to synthesize results and respond to the user query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6441fcb-b334-45fd-bcdf-7dfdc55ebde5"
   },
   "source": [
    "## Step 0: Getting Started\n",
    "\n",
    "### Install requisites: Google Vertex AI, LangChain and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ohPUPez8imvE",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform==1.47.0 in ./.local/lib/python3.10/site-packages (1.47.0)\n",
      "Requirement already satisfied: langchain==0.1.14 in ./.local/lib/python3.10/site-packages (0.1.14)\n",
      "Requirement already satisfied: langchain-google-vertexai==0.1.3 in ./.local/lib/python3.10/site-packages (0.1.3)\n",
      "Requirement already satisfied: typing_extensions==4.9.0 in ./.local/lib/python3.10/site-packages (4.9.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.47.0) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.47.0) (2.29.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.47.0) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.47.0) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.47.0) (23.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.47.0) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.47.0) (3.21.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.47.0) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.47.0) (1.8.5.post1)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.47.0) (1.10.15)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.47.0) (0.16)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.local/lib/python3.10/site-packages (from langchain==0.1.14) (0.6.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.30 in ./.local/lib/python3.10/site-packages (from langchain==0.1.14) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.37 in ./.local/lib/python3.10/site-packages (from langchain==0.1.14) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in ./.local/lib/python3.10/site-packages (from langchain==0.1.14) (0.0.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.local/lib/python3.10/site-packages (from langchain==0.1.14) (0.1.59)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.local/lib/python3.10/site-packages (from langchain==0.1.14) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (8.2.3)\n",
      "Requirement already satisfied: types-protobuf<5.0.0.0,>=4.24.0.4 in ./.local/lib/python3.10/site-packages (from langchain-google-vertexai==0.1.3) (4.25.0.20240417)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.31.0 in ./.local/lib/python3.10/site-packages (from langchain-google-vertexai==0.1.3) (2.31.0.20240406)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (0.9.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.47.0) (1.63.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.47.0) (1.63.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform==1.47.0) (1.48.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.47.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.47.0) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.47.0) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.47.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.47.0) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.47.0) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.47.0) (0.12.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.47.0) (1.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.14) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.14) (3.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.14) (3.0.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform==1.47.0) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.47.0) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (1.0.0)\n",
      "libtesseract-dev is already the newest version (4.1.1-2.1).\n",
      "tesseract-ocr is already the newest version (4.1.1-2.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n",
      "Requirement already satisfied: unstructured==0.12.4 in ./.local/lib/python3.10/site-packages (0.12.4)\n",
      "Requirement already satisfied: pdf2image==1.17.0 in ./.local/lib/python3.10/site-packages (1.17.0)\n",
      "Requirement already satisfied: pytesseract==0.3.10 in ./.local/lib/python3.10/site-packages (0.3.10)\n",
      "Collecting pdfminer.six==20221105\n",
      "  Using cached pdfminer.six-20221105-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: chardet in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (5.2.0)\n",
      "Requirement already satisfied: filetype in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (1.2.0)\n",
      "Requirement already satisfied: python-magic in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (0.4.27)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (5.2.2)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from unstructured==0.12.4) (0.9.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from unstructured==0.12.4) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from unstructured==0.12.4) (4.12.3)\n",
      "Requirement already satisfied: emoji in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (2.11.1)\n",
      "Requirement already satisfied: dataclasses-json in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (0.6.6)\n",
      "Requirement already satisfied: python-iso639 in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (2024.4.27)\n",
      "Requirement already satisfied: langdetect in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (1.0.9)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (3.9.0)\n",
      "Requirement already satisfied: backoff in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (4.9.0)\n",
      "Requirement already satisfied: unstructured-client>=0.15.1 in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (0.22.0)\n",
      "Requirement already satisfied: wrapt in ./.local/lib/python3.10/site-packages (from unstructured==0.12.4) (1.14.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from pdf2image==1.17.0) (10.3.0)\n",
      "Requirement already satisfied: packaging>=21.3 in ./.local/lib/python3.10/site-packages (from pytesseract==0.3.10) (23.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20221105) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20221105) (42.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (2024.2.2)\n",
      "Requirement already satisfied: deepdiff>=6.0 in ./.local/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (7.0.1)\n",
      "Requirement already satisfied: idna>=3.4 in /opt/conda/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (3.7)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in ./.local/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (1.0.6)\n",
      "Requirement already satisfied: marshmallow>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (3.21.2)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (1.0.0)\n",
      "Requirement already satisfied: pypdf>=4.0 in ./.local/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (4.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (2.9.0)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (1.16.0)\n",
      "Requirement already satisfied: typing-inspect>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (0.9.0)\n",
      "Requirement already satisfied: urllib3>=1.26.18 in ./.local/lib/python3.10/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.4) (2.2.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->unstructured==0.12.4) (2.5)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured==0.12.4) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured==0.12.4) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured==0.12.4) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured==0.12.4) (4.66.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105) (2.22)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in ./.local/lib/python3.10/site-packages (from deepdiff>=6.0->unstructured-client>=0.15.1->unstructured==0.12.4) (4.1.0)\n",
      "Using cached pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "Installing collected packages: pdfminer.six\n",
      "  Attempting uninstall: pdfminer.six\n",
      "    Found existing installation: pdfminer.six 20231228\n",
      "    Uninstalling pdfminer.six-20231228:\n",
      "      Successfully uninstalled pdfminer.six-20231228\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pdfplumber 0.11.0 requires pdfminer.six==20231228, but you have pdfminer-six 20221105 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pdfminer.six-20221105\n",
      "Requirement already satisfied: pillow-heif==0.15.0 in ./.local/lib/python3.10/site-packages (0.15.0)\n",
      "Requirement already satisfied: opencv-python==4.9.0.80 in ./.local/lib/python3.10/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: unstructured-inference==0.7.24 in ./.local/lib/python3.10/site-packages (0.7.24)\n",
      "Requirement already satisfied: pikepdf==8.13.0 in ./.local/lib/python3.10/site-packages (8.13.0)\n",
      "Requirement already satisfied: pypdf==4.0.1 in ./.local/lib/python3.10/site-packages (4.0.1)\n",
      "Requirement already satisfied: pillow>=9.2.0 in /opt/conda/lib/python3.10/site-packages (from pillow-heif==0.15.0) (10.3.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./.local/lib/python3.10/site-packages (from opencv-python==4.9.0.80) (1.26.4)\n",
      "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in ./.local/lib/python3.10/site-packages (from unstructured-inference==0.7.24) (0.3.4)\n",
      "Requirement already satisfied: python-multipart in ./.local/lib/python3.10/site-packages (from unstructured-inference==0.7.24) (0.0.9)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from unstructured-inference==0.7.24) (0.23.0)\n",
      "Requirement already satisfied: onnx in ./.local/lib/python3.10/site-packages (from unstructured-inference==0.7.24) (1.16.0)\n",
      "Requirement already satisfied: onnxruntime<1.16 in ./.local/lib/python3.10/site-packages (from unstructured-inference==0.7.24) (1.15.1)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /opt/conda/lib/python3.10/site-packages (from unstructured-inference==0.7.24) (4.30.1)\n",
      "Requirement already satisfied: rapidfuzz in ./.local/lib/python3.10/site-packages (from unstructured-inference==0.7.24) (3.9.0)\n",
      "Requirement already satisfied: Deprecated in /opt/conda/lib/python3.10/site-packages (from pikepdf==8.13.0) (1.2.14)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from pikepdf==8.13.0) (23.2)\n",
      "Requirement already satisfied: lxml>=4.8 in ./.local/lib/python3.10/site-packages (from pikepdf==8.13.0) (5.2.2)\n",
      "Requirement already satisfied: coloredlogs in ./.local/lib/python3.10/site-packages (from onnxruntime<1.16->unstructured-inference==0.7.24) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime<1.16->unstructured-inference==0.7.24) (24.3.25)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.10/site-packages (from onnxruntime<1.16->unstructured-inference==0.7.24) (3.20.3)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from onnxruntime<1.16->unstructured-inference==0.7.24) (1.12)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->unstructured-inference==0.7.24) (3.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->unstructured-inference==0.7.24) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->unstructured-inference==0.7.24) (2024.4.16)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->unstructured-inference==0.7.24) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->unstructured-inference==0.7.24) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->unstructured-inference==0.7.24) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.25.1->unstructured-inference==0.7.24) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->unstructured-inference==0.7.24) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub->unstructured-inference==0.7.24) (4.9.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./.local/lib/python3.10/site-packages (from Deprecated->pikepdf==8.13.0) (1.14.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (1.11.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (1.5.3)\n",
      "Requirement already satisfied: iopath in ./.local/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (0.1.10)\n",
      "Requirement already satisfied: pdfplumber in ./.local/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (0.11.0)\n",
      "Requirement already satisfied: pdf2image in ./.local/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (1.17.0)\n",
      "Requirement already satisfied: pytesseract in ./.local/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (0.3.10)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (2.3.0)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (0.18.0)\n",
      "Requirement already satisfied: effdet in ./.local/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (0.4.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.24) (10.0)\n",
      "Requirement already satisfied: timm>=0.9.2 in ./.local/lib/python3.10/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (1.0.3)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in ./.local/lib/python3.10/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (2.0.7)\n",
      "Requirement already satisfied: omegaconf>=2.0 in ./.local/lib/python3.10/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (2.3.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in ./.local/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (12.4.127)\n",
      "Requirement already satisfied: portalocker in ./.local/lib/python3.10/site-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (2.8.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (2024.1)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24)\n",
      "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in ./.local/lib/python3.10/site-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (42.0.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.25.1->unstructured-inference==0.7.24) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->transformers>=4.25.1->unstructured-inference==0.7.24) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.25.1->unstructured-inference==0.7.24) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.10/site-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.24) (1.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in ./.local/lib/python3.10/site-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (4.9.3)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (3.7.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (2.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (3.1.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.24) (2.22)\n",
      "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "Installing collected packages: pdfminer.six\n",
      "  Attempting uninstall: pdfminer.six\n",
      "    Found existing installation: pdfminer.six 20221105\n",
      "    Uninstalling pdfminer.six-20221105:\n",
      "      Successfully uninstalled pdfminer.six-20221105\n",
      "Successfully installed pdfminer.six-20231228\n",
      "Requirement already satisfied: tensorflow_hub==0.16.1 in /opt/conda/lib/python3.10/site-packages (0.16.1)\n",
      "Requirement already satisfied: tensorflow_text==2.15.0 in ./.local/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./.local/lib/python3.10/site-packages (from tensorflow_hub==0.16.1) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in ./.local/lib/python3.10/site-packages (from tensorflow_hub==0.16.1) (3.20.3)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow_hub==0.16.1) (2.15.0)\n",
      "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in ./.local/lib/python3.10/site-packages (from tensorflow_text==2.15.0) (2.15.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in ./.local/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (23.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.local/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.29.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.local/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in ./.local/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in ./.local/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in ./.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.1.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2024.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install --user --upgrade google-cloud-aiplatform==1.47.0 langchain==0.1.14 langchain-google-vertexai==0.1.3 typing_extensions==4.9.0\n",
    "\n",
    "# Dependencies required by Unstructured PDF loader\n",
    "! sudo apt -y -qq install tesseract-ocr libtesseract-dev\n",
    "! sudo apt-get -y -qq install poppler-utils\n",
    "! pip install --user --upgrade unstructured==0.12.4 pdf2image==1.17.0 pytesseract==0.3.10 pdfminer.six==20221105\n",
    "! pip install --user --upgrade pillow-heif==0.15.0 opencv-python==4.9.0.80 unstructured-inference==0.7.24 pikepdf==8.13.0 pypdf==4.0.1\n",
    "\n",
    "# For Matching Engine integration dependencies (default embeddings)\n",
    "! pip install --user --upgrade tensorflow_hub==0.16.1 tensorflow_text==2.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09720c707f1c"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BfgKzyTg7Lzh",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.47.0\n",
      "LangChain version: 0.1.14\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Utils\n",
    "import time\n",
    "import uuid\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import vertexai\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n",
    "# LangChain\n",
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import GCSDirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Import custom Matching Engine packages\n",
    "from langchain_google_vertexai import (\n",
    "    VertexAI,\n",
    "    VertexAIEmbeddings,\n",
    "    VectorSearchVectorStore,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fd69c1773887",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"genai-423417\" \n",
    "REGION = \"us-central1\"  \n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3edb33f6-1d81-4e39-a8d6-9c36644f19bb"
   },
   "source": [
    "### Initialize LangChain Models\n",
    "\n",
    "Use PaLM `text-bison@002` to initialize LangChain with the pre-trained text. \n",
    "\n",
    "FYI, for chat and embeddings generation: use `chat-bison@version-xyz` and `textembedding-gecko@version-xyz` respectively. \"Version-xyz\" is at https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eVpPcvsrkzCk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text model instance integrated with langChain\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@002\",\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0.2,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Embeddings API integrated with langChain\n",
    "embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlavSzL9moaV"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5377ed24-8408-4c3d-aa7b-49872bb1e27a"
   },
   "source": [
    "## STEP 1: Create Vector Search Index and Endpoint for the eventual retrieval\n",
    "\n",
    "(I imagine the following code will be out-of-date soon, as `Google Vector Search` is the re-branded name of `Vertex AI Matching Engine`, even though its code still calls `MatchingEngine`.)\n",
    "\n",
    "\n",
    "[Embeddings](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings) are a way of representing data as n-dimensional vector, in a space where the locations of those points in space are semantically meaningful. These embeddings can be then used to find similar data points. You can get text embeddings using [Vertex AI Embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings). These embeddings are managed using a vector database.\n",
    "\n",
    "\n",
    "[Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview) is a Google Cloud managed vector database, which stores data as high-dimensional vectors (embeddings) and can find the most similar vectors from over a billion vectors. Matching Engine's Approximate Nearest Neigbors (ANN) service can serve similarity-matching queries at high queries per second (QPS). Unlike vector stores that run locally, Matching Engine is optimized for scale (multi-million and billion vectors) and it's an enterprise ready engine.\n",
    "\n",
    "As part of the environment setup, create an index on Vertex AI Matching Engine and deploy the index to an Endpoint. Index Endpoint can be [public](https://cloud.google.com/vertex-ai/docs/matching-engine/deploy-index-public) or [private](https://cloud.google.com/vertex-ai/docs/matching-engine/deploy-index-vpc). This notebook uses a **Public endpoint**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb58ad7f-6c3c-4ca9-bb09-ffcd9719c6ec"
   },
   "source": [
    "- Configure parameters to create Matching Engine index\n",
    "    - `ME_REGION`: Region where Matching Engine Index and Index Endpoint are deployed\n",
    "    - `ME_INDEX_NAME`: Matching Engine index display name\n",
    "    - `ME_EMBEDDING_DIR`: Cloud Storage path to allow inserting, updating or deleting the contents of the Index\n",
    "    - `ME_DIMENSIONS`: The number of dimensions of the input vectors. Vertex AI Embedding API generates 768 dimensional vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "b89f8611",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ME_REGION = REGION\n",
    "ME_INDEX_NAME = f\"{PROJECT_ID}-me-index\"  \n",
    "ME_EMBEDDING_DIR = f\"{PROJECT_ID}-me-bucket\"  \n",
    "ME_DIMENSIONS = 768  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHnWpmClkc0Z"
   },
   "source": [
    "Make a Google Cloud Storage bucket for your Matching Engine index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VPYLPCxHkc0Z",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ gsutil mb -p genai-423417 -l us-central1 gs://genai-423417-me-bucket\n",
      "Creating gs://genai-423417-me-bucket/...\n"
     ]
    }
   ],
   "source": [
    "! set -x && gsutil mb -p $PROJECT_ID -l us-central1 gs://$ME_EMBEDDING_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "183f19ed-d140-4713-8788-e4e5a1f121c0"
   },
   "source": [
    "### Create Index\n",
    "\n",
    "For the record, in 2022, I had implemented Google Research: [Accelerating Large-Scale Inference with Anisotropic Vector Quantization](https://arxiv.org/abs/1908.10396).\n",
    "\n",
    "For more information about how this works, see [Announcing ScaNN: Efficient\n",
    "Vector Similarity Search](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8397f796",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/841470159359/locations/us-central1/indexes/2567812649647603712/operations/5879008993484996608\n",
      "MatchingEngineIndex created. Resource name: projects/841470159359/locations/us-central1/indexes/2567812649647603712\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/841470159359/locations/us-central1/indexes/2567812649647603712')\n",
      "2567812649647603712\n"
     ]
    }
   ],
   "source": [
    "# NOTE : This operation can take upto 30 seconds\n",
    "my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=\"langchain-index\",\n",
    "    dimensions=768,\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    index_update_method=\"STREAM_UPDATE\",  # allowed values BATCH_UPDATE , STREAM_UPDATE\n",
    ")\n",
    "if my_index:\n",
    "    print(my_index.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46e4d3b6-3f37-40b9-82ef-1a138e0fc901"
   },
   "source": [
    "### Deploy Index to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e6535ceb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/841470159359/locations/us-central1/indexEndpoints/396092466836537344/operations/6429574047931039744\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/841470159359/locations/us-central1/indexEndpoints/396092466836537344\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/841470159359/locations/us-central1/indexEndpoints/396092466836537344')\n",
      "Index endpoint resource name: 396092466836537344\n",
      "Index endpoint public domain name: 1354660318.us-central1-841470159359.vdb.vertexai.goog\n"
     ]
    }
   ],
   "source": [
    "# Create an endpoint\n",
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=f\"langchain-index-endpoint\", public_endpoint_enabled=True\n",
    ")\n",
    "if index_endpoint:\n",
    "    print(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
    "    print(\n",
    "        f\"Index endpoint public domain name: {index_endpoint.public_endpoint_domain_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Xq-pqAse4UIY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/841470159359/locations/us-central1/indexEndpoints/396092466836537344\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/841470159359/locations/us-central1/indexEndpoints/396092466836537344/operations/5928548589386072064\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/841470159359/locations/us-central1/indexEndpoints/396092466836537344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[id: \"langchain_index_endpoint_deployed_index\"\n",
       "index: \"projects/841470159359/locations/us-central1/indexes/2567812649647603712\"\n",
       "create_time {\n",
       "  seconds: 1715968357\n",
       "  nanos: 137595000\n",
       "}\n",
       "index_sync_time {\n",
       "  seconds: 1715969294\n",
       "  nanos: 428142000\n",
       "}\n",
       "automatic_resources {\n",
       "  min_replica_count: 2\n",
       "  max_replica_count: 2\n",
       "}\n",
       "deployment_group: \"default\"\n",
       "]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy Index to endpoint\n",
    "# NOTE : This operation can take upto 20 minutes\n",
    "my_index_endpoint = index_endpoint.deploy_index(\n",
    "    index=my_index, deployed_index_id=\"langchain_index_endpoint_deployed_index\"\n",
    ")\n",
    "\n",
    "my_index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtKydrDnmw9d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta-Vt4t3wTQ7"
   },
   "source": [
    "## STEP 2: Add Document Embeddings to Matching Engine - Vector Store\n",
    "\n",
    "This step ingests and parse PDF documents, split them, generate embeddings and add the embeddings to the vector store. The document corpus used as dataset is a sample of Google published research papers across different domains - large models, traffic simulation, productivity etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "520d34f2-7f90-4d74-97ae-c0f0d03ed27d"
   },
   "source": [
    "### Ingest PDF files\n",
    "\n",
    "The document corpus is hosted on Cloud Storage bucket (at `gs://github-repo/documents/google-research-pdfs/`) and LangChain provides a convenient document loader [`GCSDirectoryLoader`](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/google_cloud_storage_directory.html) to load documents from a Cloud Storage bucket. The loader uses `Unstructured` package to load files of many types including pdfs, images, html and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWLm7nNekc0b"
   },
   "source": [
    "Make a Google Cloud Storage bucket in your GCP project to copy the document files into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lNhd573Ekc0b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ gsutil mb -p genai-423417 -l us-central1 gs://genai-423417-documents\n",
      "Creating gs://genai-423417-documents/...\n"
     ]
    }
   ],
   "source": [
    "GCS_BUCKET_DOCS = f\"{PROJECT_ID}-documents\"\n",
    "! set -x && gsutil mb -p $PROJECT_ID -l us-central1 gs://$GCS_BUCKET_DOCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZcbp9Vxkc0b"
   },
   "source": [
    "Copy document files to your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JBUdfgocaMWA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/a-human-centered-approach-to-developer-productivity.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/a-mixed-methods-approach-to-understanding-user-trust-after-voice-assistant-failures.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/adaptations-of-ux-practice-to-meet-responsible-ai-challenges.pdf [Content-Type=application/pdf]...\n",
      "\\ [4 files][  5.8 MiB/  5.8 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rsync ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://github-repo/documents/google-research-pdfs/adherence-bandits.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/adversarial-streaming-via-differential-privacy-and-difference-estimators.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/annotator-diversity-in-data-practices.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/characterizing-attribution-and-fluency-tradeoffs-for-retrieval-augmented-llms.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/code-generation-for-in-place-stencils.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/connecting-vision-and-language-with-video-localized-narratives.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/cost-utility-analysis-of-deep-learning-and-trained-human-graders-for-diabetic-retinopathy.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/detecting-news-headline-hallucinations-with-explanations.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/development-of-a-ml-model-for-sonographic-assessment-of-gestational-age.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/differentially-private-continual-releases-of-streaming-freq-moment-estimations.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/how-ai-language-models-can-enhance-or-impede-communication-for-aac-users.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/increasing-impact-of-mobile-health-programs.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://github-repo/documents/google-research-pdfs/making-computer-science-data-fair.pdf [Content-Type=application/pdf]...\n",
      "\\ [20 files][ 40.6 MiB/ 40.6 MiB]                                               \n",
      "Operation completed over 20 objects/40.6 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "folder_prefix = \"documents/google-research-pdfs/\"\n",
    "!gsutil rsync -r gs://github-repo/documents/google-research-pdfs/ gs://$GCS_BUCKET_DOCS/$folder_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0f2c27354be"
   },
   "source": [
    "Load documents and add document metadata such as file name, to be retrieved later when citing the references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3QjgeWSMw0Bb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents from genai-423417-documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `GCSDirectoryLoader` was deprecated in LangChain 0.0.32 and will be removed in 0.3.0. An updated version of the class exists in the langchain-google-community package and should be used instead. To use it run `pip install -U langchain-google-community` and import as `from langchain_google_community import GCSDirectoryLoader`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents loaded (pre-chunking) = 20\n"
     ]
    }
   ],
   "source": [
    "# Ingest PDF files\n",
    "\n",
    "print(f\"Processing documents from {GCS_BUCKET_DOCS}\")\n",
    "loader = GCSDirectoryLoader(\n",
    "    project_name=PROJECT_ID, bucket=GCS_BUCKET_DOCS, prefix=folder_prefix\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# Add document name and source to the metadata\n",
    "for document in documents:\n",
    "    doc_md = document.metadata\n",
    "    document_name = doc_md[\"source\"].split(\"/\")[-1]\n",
    "    # derive doc source from Document loader\n",
    "    doc_source_prefix = \"/\".join(GCS_BUCKET_DOCS.split(\"/\")[:3])\n",
    "    doc_source_suffix = \"/\".join(doc_md[\"source\"].split(\"/\")[4:-1])\n",
    "    source = f\"{doc_source_prefix}/{doc_source_suffix}\"\n",
    "    document.metadata = {\"source\": source, \"document_name\": document_name}\n",
    "\n",
    "print(f\"# of documents loaded (pre-chunking) = {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0029f79c92e3"
   },
   "source": [
    "Verify document metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "a900426d952c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'genai-423417-documents/google-research-pdfs',\n",
       " 'document_name': 'a-human-centered-approach-to-developer-productivity.pdf'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dbeedfd-b68f-43e2-b888-4b4b3b258f47"
   },
   "source": [
    "### Chunk documents\n",
    "\n",
    "Split the documents to smaller chunks. When splitting the document, ensure a few chunks can fit within the context length of LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OJecKul0xgWT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents = 2091\n"
     ]
    }
   ],
   "source": [
    "# split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add chunk number to metadata\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"chunk\"] = idx\n",
    "\n",
    "print(f\"# of documents = {len(doc_splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66e0493f-deae-4a53-ae1f-b7eb8fc1c50d"
   },
   "source": [
    "### Configure Matching Engine as Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ee88a89-a8ff-40c4-bd9a-fe46e6dd40b4"
   },
   "source": [
    "Initialize Matching Engine vector store with text embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7125956a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize vector store\n",
    "me = VectorSearchVectorStore.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=ME_REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_DIR}\".split(\"/\")[2],\n",
    "    embedding=embeddings,\n",
    "    index_id=my_index.name,\n",
    "    endpoint_id=index_endpoint.name,\n",
    "    stream_update=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2926d5ed-495d-443e-9b7d-de20d46f7ba5"
   },
   "source": [
    "### Add documents as embeddings in Matching Engine as index\n",
    "\n",
    "The document chunks are transformed as embeddings (vectors) using Vertex AI Embeddings API and added to the index with **[streaming index update](https://cloud.google.com/vertex-ai/docs/matching-engine/create-manage-index#create-index)**. With Streaming Updates, you can update and query your index within a few seconds.\n",
    "\n",
    "The original document text is stored on Cloud Storage bucket had referenced by id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd760fc2c21e"
   },
   "source": [
    "Prepare text and metadata to be added to the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bcdfd391",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store docs as embeddings in Matching Engine index\n",
    "# It may take a while since API is rate limited\n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [doc.metadata for doc in doc_splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b56367393115"
   },
   "source": [
    "Add embeddings to the vector store\n",
    "\n",
    "**NOTE:** Depending on the volume and size of documents, this step may take time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4fbe5dda53b8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting datapoints MatchingEngineIndex index: projects/841470159359/locations/us-central1/indexes/2567812649647603712\n",
      "MatchingEngineIndex index Upserted datapoints. Resource name: projects/841470159359/locations/us-central1/indexes/2567812649647603712\n",
      "Upserting datapoints MatchingEngineIndex index: projects/841470159359/locations/us-central1/indexes/2567812649647603712\n",
      "MatchingEngineIndex index Upserted datapoints. Resource name: projects/841470159359/locations/us-central1/indexes/2567812649647603712\n",
      "Upserting datapoints MatchingEngineIndex index: projects/841470159359/locations/us-central1/indexes/2567812649647603712\n",
      "MatchingEngineIndex index Upserted datapoints. Resource name: projects/841470159359/locations/us-central1/indexes/2567812649647603712\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(texts), 1000):\n",
    "    me.add_texts(texts[i : i + 1000], metadatas[i : i + 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9704a63981a1"
   },
   "source": [
    "Validate semantic search with Matching Engine is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Y8tNk0jQPabQ",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Abstract\\n\\nWe propose Video Localized Narratives, a new form of multimodal video annotations connecting vision and lan- guage. In the original Localized Narratives [40], annota- tors speak and move their mouse simultaneously on an im- age, thus grounding each word with a mouse trace segment. However, this is challenging on a video. Our new protocol empowers annotators to tell the story of a video with Local- ized Narratives, capturing even complex events involving multiple actors interacting with each other and with sev- eral passive objects. We annotated 20k videos of the OVIS, UVO, and Oops datasets, totalling 1.7M words. Based on this data, we also construct new benchmarks for the video narrative grounding and video question answering tasks, and provide reference results from strong baseline models. Our annotations are available at https://google. github.io/video-localized-narratives/.', metadata={'source': 'genai-423417-documents/google-research-pdfs', 'document_name': 'connecting-vision-and-language-with-video-localized-narratives.pdf', 'chunk': 1154}),\n",
       " Document(page_content='Vision and language is a very active research area, which experienced much exciting progress recently [1, 32, 42, 43, 52]. At the heart of many developments lie datasets con- necting still images to captions, while grounding some of the words in the caption to regions in the image, made with a variety of protocols [6,21,24,26,33,38,40,48,50,60]. The recent Localized Narratives (ImLNs) [40] offer a particu- larly attractive solution: the annotators describe an image with their voice while simultaneously moving their mouse over the regions they are describing. Speaking is natural and efﬁcient, resulting in long captions that describe the whole scene. Moreover, the synchronization between the voice and mouse pointer yields dense visual grounding for every word. Yet, still images only show one instant in time. Anno- tating videos would be even more interesting, as they show entire stories, featuring a ﬂow of events involving multiple actors and objects interacting with each other.', metadata={'source': 'genai-423417-documents/google-research-pdfs', 'document_name': 'connecting-vision-and-language-with-video-localized-narratives.pdf', 'chunk': 1158})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test whether search from vector store is working\n",
    "me.similarity_search(\"What are video localized narratives?\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "d9c0a89c8ac7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='<latexit sha1_base64=\"+t0/fCT+bwdn8UYzdjxaOo9NJME=\">AAACPHicfZDLSsNAFIYn9V6v1aWbYBVFpCQi6rKgC12IF2wrNKWcTE/SoZNJmJmINfQN3Orj+B7u3Ylb105rBW94YODjP//MnPP7CWdKO86TlRsZHRufmJzKT8/Mzs0vFBarKk4lxQqNeSyvfFDImcCKZprjVSIRIp9jze8c9Pu1a5SKxeJSdxNsRBAKFjAK2kgXt+vNhaJTcgZl/wZ3CEUyrLNmwVr1WjFNIxSaclCq7jqJbmQgNaMce3kvVZgA7UCIdYMCIlSNbDBqz14zSssOYmmO0PZA/Xojg0ipbuQbZwS6rX72+uJfvXqqg/1GxkSSahT046Mg5baO7f7edotJpJp3DQCVzMxq0zZIoNqkk/cO0ewi8cS8e5qgBB3LzcwDGUZw0zO7hd5Wn/4zMvFpNJQ3ubo/U/wN1e2Su1vaOd8plo+HCU+SZbJCNohL9kiZHJEzUiGUBOSO3JMH69F6tl6s1w9rzhreWSLfynp7By8brgI=</latexit>z0', metadata={'source': 'genai-423417-documents/google-research-pdfs', 'document_name': 'a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf', 'chunk': 206}),\n",
       " Document(page_content='<latexit sha1_base64=\"mPpLqvfiTr3sYUH3/8vodE3xifA=\">AAACPHicfZDLSgMxFIYzXmu969JNsAoiUmakqMuCLnQhXrC10ClyJj3ThmYyQ5IR69A3cKuP43u4dyduXZvWCt7wQODjP3+Sc/4gEVwb131yRkbHxicmc1P56ZnZufmFxaWqjlPFsMJiEataABoFl1gx3AisJQohCgReBp39fv/yGpXmsbww3QQbEbQkDzkDY6XzW3q1UHCL7qDob/CGUCDDOr1adNb8ZszSCKVhArSue25iGhkow5nAXt5PNSbAOtDCukUJEepGNhi1R9et0qRhrOyRhg7UrzcyiLTuRoF1RmDa+mevL/7Vq6cm3GtkXCapQck+PgpTQU1M+3vTJlfIjOhaAKa4nZWyNihgxqaT9w/Q7qLw2L57kqACE6vNzAfViuCmZ3dr+Vt9+s/I5afRUt7m6v1M8TdUt4veTrF0ViqUj4YJ58gKWSUbxCO7pEwOySmpEEZCckfuyYPz6Dw7L87rh3XEGd5ZJt/KeXsHIjqt+w==</latexit>z', metadata={'source': 'genai-423417-documents/google-research-pdfs', 'document_name': 'a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf', 'chunk': 204})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me.similarity_search(\"What is NFC?\", k=2, search_distance=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EisLfGPjnRCb"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjaHbVCanMFE"
   },
   "source": [
    "## STEP 3: Retrieval based Question/Answering Chain\n",
    "\n",
    "LangChain provides easy ways to chain multiple tasks that can do QA over a set of documents, called QA chains. The notebook works with [**RetrievalQA**](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html) chain which is based on **load_qa_chain** under the hood.\n",
    "\n",
    "In the retrieval augmented generation chain, the Matching Engine uses semantic search to retrieve relevant documents based on the user's question. The resulting documents are then added as additional context to the prompt sent to the LLM, along with the user's question, to generate a response. Thus the response generated by LLM is grounded to your documents in the corpus.\n",
    "\n",
    "This way, a user would only need to provide their question as a prompt and the retrieval chain would be able to seek the answers using Matching Engine directly, and return a proper text response answering the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "896c9161-e03b-4bdd-8714-186830374758"
   },
   "source": [
    "### Configure Question/Answering Chain with Vector Store using Text\n",
    "\n",
    "Define Matching Engine Vector Store as retriever that takes in a query and returns a list of relevant documents. The retriever implementation supports configuring number of documents to fetch and filtering by search distance as a threshold value parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4535f0972145",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create chain to answer questions\n",
    "NUMBER_OF_RESULTS = 10\n",
    "SEARCH_DISTANCE_THRESHOLD = 0.6\n",
    "\n",
    "# Expose index to the retriever\n",
    "retriever = me.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": NUMBER_OF_RESULTS,\n",
    "        \"search_distance\": SEARCH_DISTANCE_THRESHOLD,\n",
    "    },\n",
    "    filters=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bc30daac201"
   },
   "source": [
    "Customize the default retrieval prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ibQdjXjw1foF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"SYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
    "\n",
    "Do not try to make up an answer:\n",
    " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
    " - If the context is empty, just say \"I do not know the answer to that.\"\n",
    "\n",
    "=============\n",
    "{context}\n",
    "=============\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "840b8b89371a"
   },
   "source": [
    "Configure RetrievalQA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9551d95f173b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uses LLM to synthesize results from the search index.\n",
    "# Use Vertex PaLM Text API for LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3289f85d378"
   },
   "source": [
    "Enable verbose logging for debugging and troubleshooting the chains which includes the complete prompt to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "e4ZaWndxQ6nN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable for troubleshooting\n",
    "qa.combine_documents_chain.verbose = True\n",
    "qa.combine_documents_chain.llm_chain.verbose = True\n",
    "qa.combine_documents_chain.llm_chain.llm.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88ca4ff90ec8"
   },
   "source": [
    "Utility function to format the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gz3NyWTTPsrH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def formatter(result):\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(\".\" * 80)\n",
    "    if \"source_documents\" in result.keys():\n",
    "        for idx, ref in enumerate(result[\"source_documents\"]):\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"REFERENCE #{idx}\")\n",
    "            print(\"-\" * 80)\n",
    "            if \"score\" in ref.metadata:\n",
    "                print(f\"Matching Score: {ref.metadata['score']}\")\n",
    "            if \"source\" in ref.metadata:\n",
    "                print(f\"Document Source: {ref.metadata['source']}\")\n",
    "            if \"document_name\" in ref.metadata:\n",
    "                print(f\"Document Name: {ref.metadata['document_name']}\")\n",
    "            print(\".\" * 80)\n",
    "            print(f\"Content: \\n{wrap(ref.page_content)}\")\n",
    "    print(\".\" * 80)\n",
    "    print(f\"Response: {wrap(result['result'])}\")\n",
    "    print(\".\" * 80)\n",
    "\n",
    "\n",
    "def wrap(s):\n",
    "    return \"\\n\".join(textwrap.wrap(s, width=120, break_long_words=False))\n",
    "\n",
    "\n",
    "def ask(\n",
    "    query,\n",
    "    qa=qa,\n",
    "    k=NUMBER_OF_RESULTS,\n",
    "    search_distance=SEARCH_DISTANCE_THRESHOLD,\n",
    "    filters={},\n",
    "):\n",
    "    qa.retriever.search_kwargs[\"search_distance\"] = search_distance\n",
    "    qa.retriever.search_kwargs[\"k\"] = k\n",
    "    qa.retriever.search_kwargs[\"filters\"] = filters\n",
    "    result = qa({\"query\": query})\n",
    "    return formatter(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9381b7a8f24"
   },
   "source": [
    "### Run QA chain on sample questions\n",
    "\n",
    "Following are sample questions you could try. Wehn you run the query, RetrievalQA chain takes the user question, call the retriever to fetch top *k* semantically similar texts from the Matching Engine Index (vector store) and passes to the LLM as part of the prompt. The final prompt sent to the LLM looks of this format:\n",
    "\n",
    "```\n",
    "SYSTEM: {system}\n",
    "\n",
    "=============\n",
    "{context}\n",
    "=============\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "```\n",
    "\n",
    "where:\n",
    " - `system`: Instructions for LLM on how to respond to the question based on the context\n",
    " - `context`: Semantically similar text (a.k.a snippets) retreived from the vector store\n",
    " - `question`: question posed by the user\n",
    "\n",
    "\n",
    "The response returned from the LLM includes both the response and references that lead to the response. This way the response from LLM is always grounded to the sources. Here we have formatted the response as:\n",
    "\n",
    "```\n",
    "Question: {question}\n",
    "--------------------------------------------------------------------------------\n",
    "REFERENCE #n\n",
    "--------------------------------------------------------------------------------\n",
    "Matching Score: <score>\n",
    "Document Source: <document source location>\n",
    "Document Name: <document file name>\n",
    "................................................................................\n",
    "Context:\n",
    "{}\n",
    "................................................................................\n",
    "Response: <answer returned by the LLM>\n",
    "................................................................................\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "09cb0f4c981a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
      "\n",
      "Question: What are video localized narratives?\n",
      "\n",
      "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
      "\n",
      "Do not try to make up an answer:\n",
      " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
      " - If the context is empty, just say \"I do not know the answer to that.\"\n",
      "\n",
      "=============\n",
      "Abstract\n",
      "\n",
      "We propose Video Localized Narratives, a new form of multimodal video annotations connecting vision and lan- guage. In the original Localized Narratives [40], annota- tors speak and move their mouse simultaneously on an im- age, thus grounding each word with a mouse trace segment. However, this is challenging on a video. Our new protocol empowers annotators to tell the story of a video with Local- ized Narratives, capturing even complex events involving multiple actors interacting with each other and with sev- eral passive objects. We annotated 20k videos of the OVIS, UVO, and Oops datasets, totalling 1.7M words. Based on this data, we also construct new benchmarks for the video narrative grounding and video question answering tasks, and provide reference results from strong baseline models. Our annotations are available at https://google. github.io/video-localized-narratives/.\n",
      "\n",
      "Vision and language is a very active research area, which experienced much exciting progress recently [1, 32, 42, 43, 52]. At the heart of many developments lie datasets con- necting still images to captions, while grounding some of the words in the caption to regions in the image, made with a variety of protocols [6,21,24,26,33,38,40,48,50,60]. The recent Localized Narratives (ImLNs) [40] offer a particu- larly attractive solution: the annotators describe an image with their voice while simultaneously moving their mouse over the regions they are describing. Speaking is natural and efﬁcient, resulting in long captions that describe the whole scene. Moreover, the synchronization between the voice and mouse pointer yields dense visual grounding for every word. Yet, still images only show one instant in time. Anno- tating videos would be even more interesting, as they show entire stories, featuring a ﬂow of events involving multiple actors and objects interacting with each other.\n",
      "\n",
      "3 2 0 2\n",
      "\n",
      "r a\n",
      "\n",
      "M 5 1\n",
      "\n",
      "]\n",
      "\n",
      "V C . s c [\n",
      "\n",
      "2 v 7 1 2 1 1 . 2 0 3 2 : v i X r a\n",
      "\n",
      "Connecting Vision and Language with Video Localized Narratives\n",
      "\n",
      "Paul Voigtlaender\n",
      "\n",
      "Soravit Changpinyo\n",
      "\n",
      "Jordi Pont-Tuset Radu Soricut Vittorio Ferrari\n",
      "\n",
      "Google Research {voigtlaender,schangpi,jponttuset,rsoricut,vittoferrari}@google.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "We annotated the OVIS [41], UVO [51], and Oops [12] datasets with Video Localized Narratives (VidLNs). These datasets cover a general domain, as opposed to only cook- ing [10, 66] or ﬁrst-person videos [16]. Moreover, these videos contain complex scenes with interactions between multiple actors and passive objects, leading to interesting stories that are captured by our rich annotations. In to- tal our annotations span 20k videos, 72k actors, and 1.7 million words. On average, each narrative features tran- scriptions for 3.5 actors, totaling 75.1 words, including 23.0 nouns, 9.5 verbs, and 8.5 adjectives (Fig. 4). Our analysis demonstrates the high quality of the data. The text descrip- tions almost always mention objects/actions that are actu- ally present in the video, and the mouse traces do lie inside their corresponding object in most cases.\n",
      "\n",
      "Directly extending ImLNs to video by letting the anno- tator move their mouse and talk while the video is playing would lead to a “race against time”, likely resulting in fol- lowing only one salient object. In this paper, we propose\n",
      "\n",
      "The richness of our VidLNs makes them a strong ba- sis for several tasks. We demonstrate this by creating new benchmarks for the tasks of Video Narrative Grounding (VNG) and Video Question Answering (VideoQA). The new VNG task requires a method to localize the nouns in\n",
      "\n",
      "1\n",
      "\n",
      "1. Watch2./3. Select actors and key-frames5. Transcription4. Speak and move mouse Background\n",
      "\n",
      "Figure 1. The ﬁve steps of the annotation process for VidLNs. We are able to cleanly describe even complex interactions and events by disentangling the storylines of different actors.\n",
      "\n",
      "to our\n",
      "\n",
      "location-output questions are the VideoQA datasets that ground parts of their textual ques- tions and answers (e.g., noun phrases) with bounding boxes on the video [8, 27, 62]. Closest to ours is TVQA+ [8], where the task is to choose one answer out of ﬁve and ground it spatially and temporally, on videos from a single TV show. In contrast, our answers can refer to any object,\n",
      "\n",
      "Related to our\n",
      "\n",
      "3\n",
      "\n",
      "Dataset\n",
      "\n",
      "#Videos\n",
      "\n",
      "#Narratives\n",
      "\n",
      "#Actors (per narr.)\n",
      "\n",
      "#Words (per narr.)\n",
      "\n",
      "Domain\n",
      "\n",
      "Grounding\n",
      "\n",
      "OVIS-VidLN UVO-VidLN Oops-VidLN\n",
      "\n",
      "607 7,588 12,128\n",
      "\n",
      "610 8,587 12,894\n",
      "\n",
      "1,799 (2.95) 25,755 (3.00) 44,422 (3.45)\n",
      "\n",
      "28,676 (47.01) 549,303 (63.97) 1,080,211 (83.78)\n",
      "\n",
      "General General General\n",
      "\n",
      "Every word Every word Every word\n",
      "\n",
      "VidLN All\n",
      "\n",
      "20,323\n",
      "\n",
      "22,091\n",
      "\n",
      "71,976 (3.54)\n",
      "\n",
      "1,658,190 (75.06)\n",
      "\n",
      "General\n",
      "\n",
      "Every word\n",
      "\n",
      "ActivityNet-Ent [65] MPII-MD [44] YouCook2-BB [66]\n",
      "\n",
      "14,926 94 667\n",
      "\n",
      "14,926 94 667\n",
      "\n",
      "- -\n",
      "\n",
      "General 647,814 (6,891.7) Movies ∼40,500 (60.72) Cooking\n",
      "\n",
      "745,876 (49.97)\n",
      "\n",
      "432 nouns People names 67 objects\n",
      "\n",
      "In Fig. 4, we compare the per-narrative counts for the 5 main word types to those of ActivityNet-Entities [65], the most related video dataset with grounded captions. On aver- age, our narratives have 75.1 words (including 23.0 nouns, 9.5 verbs, 8.5 adjectives, 7.2 adpositions, 2.4 pronouns). Thus our narratives are longer, and contain more words of each type, than those in ActivityNet-Entities. In summary, we present the largest and richest collection of general- domain video datasets with grounded captions to date. Semantic Accuracy. We evaluate manually how well the verbs and noun phrases of VidLN describe the actions and objects in the video. We randomly picked 70 videos in each dataset and checked for every verb and noun phrase whether the described object/action is present in the video, and if it is correctly described. Tab. 2 shows the results, demonstrating that the captions are almost perfectly semantically accurate. Localization Accuracy\n",
      "\n",
      "Figure 2. An example VidLN annotation. Each row shows the story of the video from the perspective of a different actor. Each mouse trace segment drawn on selected key-frames localizes the word highlighted in the same color. More examples are in Sec. S1.\n",
      "\n",
      "an input narrative with a segmentation mask on the video frames. This is a complex task that goes beyond open- vocabulary semantic segmentation [14], as often the text has multiple identical nouns that need to be disambiguated us- ing the context provided by other words. We construct two VNG benchmarks on OVIS and UVO, with a total of 8k videos involving 45k objects. We also build baseline mod- els that explicitly attempt this disambiguation and report ex- periments showing it’s beneﬁcial on this dataset, while also highlighting these new benchmarks are far from solved.\n",
      "\n",
      "5\n",
      "\n",
      "Accuracy (%)\n",
      "\n",
      "OVIS-VLN\n",
      "\n",
      "UVO-VLN\n",
      "\n",
      "Oops-VLN\n",
      "\n",
      "Noun phrases Verbs\n",
      "\n",
      "97.7 96.0\n",
      "\n",
      "96.8 97.8\n",
      "\n",
      "97.2 97.9\n",
      "\n",
      "Table 2. Semantic accuracy for noun phrases and verbs of our three VidLN datasets.\n",
      "\n",
      "shared with us). For our VidLNs, the precision measured on bounding boxes is 83.1% (73.2% measured on segmenta- tion masks). This means that our improvements in the anno- tation protocol have been very successful, and that VidLN traces have much higher quality than ImLN traces (+25%).\n",
      "\n",
      "4. Video Narrative Grounding (VNG)\n",
      "\n",
      "[67] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional videos. In American Ass. of Art. Intelligence, 2018. 3\n",
      "\n",
      "11\n",
      "\n",
      "Appendix\n",
      "\n",
      "S1. More Video Localized Narratives Exam-\n",
      "\n",
      "ples\n",
      "\n",
      "In Figs. S1 to S3, we show more examples of our VidLN\n",
      "\n",
      "annotations.\n",
      "\n",
      "S2. ReferFormer-VNG\n",
      "\n",
      "Here we describe implementation details and additional\n",
      "\n",
      "experiments for ReferFormer-VNG.\n",
      "\n",
      "[31] (“VideoSwin”), for most setups also achieves stronger results than the ResNet-50 backbone. However, when ini- tializing from ImageNet-1k and not training on UVO-VNG, then no video data (or only for OVIS-VNG ﬁne-tuning) is used. In this case, the Video Swin Transformer which is speciﬁcally designed for videos, cannot unfold its full potential and yields relatively weak results. When initializ- ing from Kinetics, and using UVO-VNG for main-training with videos, the VideoSwin results are strong, close to the results of Swin-Large.\n",
      "\n",
      "Using instead a Video Swin Transformer\n",
      "=============\n",
      "\n",
      "Question: What are video localized narratives?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query: What are video localized narratives?\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #0\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Abstract  We propose Video Localized Narratives, a new form of multimodal video annotations connecting vision and lan-\n",
      "guage. In the original Localized Narratives [40], annota- tors speak and move their mouse simultaneously on an im- age,\n",
      "thus grounding each word with a mouse trace segment. However, this is challenging on a video. Our new protocol empowers\n",
      "annotators to tell the story of a video with Local- ized Narratives, capturing even complex events involving multiple\n",
      "actors interacting with each other and with sev- eral passive objects. We annotated 20k videos of the OVIS, UVO, and\n",
      "Oops datasets, totalling 1.7M words. Based on this data, we also construct new benchmarks for the video narrative\n",
      "grounding and video question answering tasks, and provide reference results from strong baseline models. Our annotations\n",
      "are available at https://google. github.io/video-localized-narratives/.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #1\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Vision and language is a very active research area, which experienced much exciting progress recently [1, 32, 42, 43,\n",
      "52]. At the heart of many developments lie datasets con- necting still images to captions, while grounding some of the\n",
      "words in the caption to regions in the image, made with a variety of protocols [6,21,24,26,33,38,40,48,50,60]. The\n",
      "recent Localized Narratives (ImLNs) [40] offer a particu- larly attractive solution: the annotators describe an image\n",
      "with their voice while simultaneously moving their mouse over the regions they are describing. Speaking is natural and\n",
      "efﬁcient, resulting in long captions that describe the whole scene. Moreover, the synchronization between the voice and\n",
      "mouse pointer yields dense visual grounding for every word. Yet, still images only show one instant in time. Anno-\n",
      "tating videos would be even more interesting, as they show entire stories, featuring a ﬂow of events involving multiple\n",
      "actors and objects interacting with each other.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #2\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "3 2 0 2  r a  M 5 1  ]  V C . s c [  2 v 7 1 2 1 1 . 2 0 3 2 : v i X r a  Connecting Vision and Language with Video\n",
      "Localized Narratives  Paul Voigtlaender  Soravit Changpinyo  Jordi Pont-Tuset Radu Soricut Vittorio Ferrari  Google\n",
      "Research {voigtlaender,schangpi,jponttuset,rsoricut,vittoferrari}@google.com  Abstract\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #3\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "We annotated the OVIS [41], UVO [51], and Oops [12] datasets with Video Localized Narratives (VidLNs). These datasets\n",
      "cover a general domain, as opposed to only cook- ing [10, 66] or ﬁrst-person videos [16]. Moreover, these videos contain\n",
      "complex scenes with interactions between multiple actors and passive objects, leading to interesting stories that are\n",
      "captured by our rich annotations. In to- tal our annotations span 20k videos, 72k actors, and 1.7 million words. On\n",
      "average, each narrative features tran- scriptions for 3.5 actors, totaling 75.1 words, including 23.0 nouns, 9.5 verbs,\n",
      "and 8.5 adjectives (Fig. 4). Our analysis demonstrates the high quality of the data. The text descrip- tions almost\n",
      "always mention objects/actions that are actu- ally present in the video, and the mouse traces do lie inside their\n",
      "corresponding object in most cases.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #4\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Directly extending ImLNs to video by letting the anno- tator move their mouse and talk while the video is playing would\n",
      "lead to a “race against time”, likely resulting in fol- lowing only one salient object. In this paper, we propose  The\n",
      "richness of our VidLNs makes them a strong ba- sis for several tasks. We demonstrate this by creating new benchmarks for\n",
      "the tasks of Video Narrative Grounding (VNG) and Video Question Answering (VideoQA). The new VNG task requires a method\n",
      "to localize the nouns in  1  1. Watch2./3. Select actors and key-frames5. Transcription4. Speak and move mouse\n",
      "Background  Figure 1. The ﬁve steps of the annotation process for VidLNs. We are able to cleanly describe even complex\n",
      "interactions and events by disentangling the storylines of different actors.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #5\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "to our  location-output questions are the VideoQA datasets that ground parts of their textual ques- tions and answers\n",
      "(e.g., noun phrases) with bounding boxes on the video [8, 27, 62]. Closest to ours is TVQA+ [8], where the task is to\n",
      "choose one answer out of ﬁve and ground it spatially and temporally, on videos from a single TV show. In contrast, our\n",
      "answers can refer to any object,  Related to our  3  Dataset  #Videos  #Narratives  #Actors (per narr.)  #Words (per\n",
      "narr.)  Domain  Grounding  OVIS-VidLN UVO-VidLN Oops-VidLN  607 7,588 12,128  610 8,587 12,894  1,799 (2.95) 25,755\n",
      "(3.00) 44,422 (3.45)  28,676 (47.01) 549,303 (63.97) 1,080,211 (83.78)  General General General  Every word Every word\n",
      "Every word  VidLN All  20,323  22,091  71,976 (3.54)  1,658,190 (75.06)  General  Every word  ActivityNet-Ent [65] MPII-\n",
      "MD [44] YouCook2-BB [66]  14,926 94 667  14,926 94 667  - -  General 647,814 (6,891.7) Movies ∼40,500 (60.72) Cooking\n",
      "745,876 (49.97)  432 nouns People names 67 objects\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #6\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "In Fig. 4, we compare the per-narrative counts for the 5 main word types to those of ActivityNet-Entities [65], the most\n",
      "related video dataset with grounded captions. On aver- age, our narratives have 75.1 words (including 23.0 nouns, 9.5\n",
      "verbs, 8.5 adjectives, 7.2 adpositions, 2.4 pronouns). Thus our narratives are longer, and contain more words of each\n",
      "type, than those in ActivityNet-Entities. In summary, we present the largest and richest collection of general- domain\n",
      "video datasets with grounded captions to date. Semantic Accuracy. We evaluate manually how well the verbs and noun\n",
      "phrases of VidLN describe the actions and objects in the video. We randomly picked 70 videos in each dataset and checked\n",
      "for every verb and noun phrase whether the described object/action is present in the video, and if it is correctly\n",
      "described. Tab. 2 shows the results, demonstrating that the captions are almost perfectly semantically accurate.\n",
      "Localization Accuracy\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #7\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Figure 2. An example VidLN annotation. Each row shows the story of the video from the perspective of a different actor.\n",
      "Each mouse trace segment drawn on selected key-frames localizes the word highlighted in the same color. More examples\n",
      "are in Sec. S1.  an input narrative with a segmentation mask on the video frames. This is a complex task that goes\n",
      "beyond open- vocabulary semantic segmentation [14], as often the text has multiple identical nouns that need to be\n",
      "disambiguated us- ing the context provided by other words. We construct two VNG benchmarks on OVIS and UVO, with a total\n",
      "of 8k videos involving 45k objects. We also build baseline mod- els that explicitly attempt this disambiguation and\n",
      "report ex- periments showing it’s beneﬁcial on this dataset, while also highlighting these new benchmarks are far from\n",
      "solved.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #8\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "5  Accuracy (%)  OVIS-VLN  UVO-VLN  Oops-VLN  Noun phrases Verbs  97.7 96.0  96.8 97.8  97.2 97.9  Table 2. Semantic\n",
      "accuracy for noun phrases and verbs of our three VidLN datasets.  shared with us). For our VidLNs, the precision\n",
      "measured on bounding boxes is 83.1% (73.2% measured on segmenta- tion masks). This means that our improvements in the\n",
      "anno- tation protocol have been very successful, and that VidLN traces have much higher quality than ImLN traces (+25%).\n",
      "4. Video Narrative Grounding (VNG)\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #9\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "[67] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional\n",
      "videos. In American Ass. of Art. Intelligence, 2018. 3  11  Appendix  S1. More Video Localized Narratives Exam-  ples\n",
      "In Figs. S1 to S3, we show more examples of our VidLN  annotations.  S2. ReferFormer-VNG  Here we describe\n",
      "implementation details and additional  experiments for ReferFormer-VNG.  [31] (“VideoSwin”), for most setups also\n",
      "achieves stronger results than the ResNet-50 backbone. However, when ini- tializing from ImageNet-1k and not training on\n",
      "UVO-VNG, then no video data (or only for OVIS-VNG ﬁne-tuning) is used. In this case, the Video Swin Transformer which is\n",
      "speciﬁcally designed for videos, cannot unfold its full potential and yields relatively weak results. When initializ-\n",
      "ing from Kinetics, and using UVO-VNG for main-training with videos, the VideoSwin results are strong, close to the\n",
      "results of Swin-Large.  Using instead a Video Swin Transformer\n",
      "................................................................................\n",
      "Response:  Video Localized Narratives (VidLNs) are a new form of multimodal video annotations that connect vision and language.\n",
      "They empower annotators to tell the story of a video with Localized Narratives, capturing even complex events involving\n",
      "multiple actors interacting with each other and with several passive objects.\n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "ask(\"What are video localized narratives?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0661bbcdf46"
   },
   "source": [
    "- Let's try the same question filtering on a specific document. Note that the document used in the filter does not address the question. Refer to [Vertex AI Vector Search documentation](https://cloud.google.com/vertex-ai/docs/vector-search/filtering) for the filter syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5a314a1c5dc8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
      "\n",
      "Question: What are video localized narratives?\n",
      "\n",
      "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
      "\n",
      "Do not try to make up an answer:\n",
      " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
      " - If the context is empty, just say \"I do not know the answer to that.\"\n",
      "\n",
      "=============\n",
      "Abstract\n",
      "\n",
      "We propose Video Localized Narratives, a new form of multimodal video annotations connecting vision and lan- guage. In the original Localized Narratives [40], annota- tors speak and move their mouse simultaneously on an im- age, thus grounding each word with a mouse trace segment. However, this is challenging on a video. Our new protocol empowers annotators to tell the story of a video with Local- ized Narratives, capturing even complex events involving multiple actors interacting with each other and with sev- eral passive objects. We annotated 20k videos of the OVIS, UVO, and Oops datasets, totalling 1.7M words. Based on this data, we also construct new benchmarks for the video narrative grounding and video question answering tasks, and provide reference results from strong baseline models. Our annotations are available at https://google. github.io/video-localized-narratives/.\n",
      "\n",
      "Vision and language is a very active research area, which experienced much exciting progress recently [1, 32, 42, 43, 52]. At the heart of many developments lie datasets con- necting still images to captions, while grounding some of the words in the caption to regions in the image, made with a variety of protocols [6,21,24,26,33,38,40,48,50,60]. The recent Localized Narratives (ImLNs) [40] offer a particu- larly attractive solution: the annotators describe an image with their voice while simultaneously moving their mouse over the regions they are describing. Speaking is natural and efﬁcient, resulting in long captions that describe the whole scene. Moreover, the synchronization between the voice and mouse pointer yields dense visual grounding for every word. Yet, still images only show one instant in time. Anno- tating videos would be even more interesting, as they show entire stories, featuring a ﬂow of events involving multiple actors and objects interacting with each other.\n",
      "\n",
      "3 2 0 2\n",
      "\n",
      "r a\n",
      "\n",
      "M 5 1\n",
      "\n",
      "]\n",
      "\n",
      "V C . s c [\n",
      "\n",
      "2 v 7 1 2 1 1 . 2 0 3 2 : v i X r a\n",
      "\n",
      "Connecting Vision and Language with Video Localized Narratives\n",
      "\n",
      "Paul Voigtlaender\n",
      "\n",
      "Soravit Changpinyo\n",
      "\n",
      "Jordi Pont-Tuset Radu Soricut Vittorio Ferrari\n",
      "\n",
      "Google Research {voigtlaender,schangpi,jponttuset,rsoricut,vittoferrari}@google.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "We annotated the OVIS [41], UVO [51], and Oops [12] datasets with Video Localized Narratives (VidLNs). These datasets cover a general domain, as opposed to only cook- ing [10, 66] or ﬁrst-person videos [16]. Moreover, these videos contain complex scenes with interactions between multiple actors and passive objects, leading to interesting stories that are captured by our rich annotations. In to- tal our annotations span 20k videos, 72k actors, and 1.7 million words. On average, each narrative features tran- scriptions for 3.5 actors, totaling 75.1 words, including 23.0 nouns, 9.5 verbs, and 8.5 adjectives (Fig. 4). Our analysis demonstrates the high quality of the data. The text descrip- tions almost always mention objects/actions that are actu- ally present in the video, and the mouse traces do lie inside their corresponding object in most cases.\n",
      "\n",
      "Directly extending ImLNs to video by letting the anno- tator move their mouse and talk while the video is playing would lead to a “race against time”, likely resulting in fol- lowing only one salient object. In this paper, we propose\n",
      "\n",
      "The richness of our VidLNs makes them a strong ba- sis for several tasks. We demonstrate this by creating new benchmarks for the tasks of Video Narrative Grounding (VNG) and Video Question Answering (VideoQA). The new VNG task requires a method to localize the nouns in\n",
      "\n",
      "1\n",
      "\n",
      "1. Watch2./3. Select actors and key-frames5. Transcription4. Speak and move mouse Background\n",
      "\n",
      "Figure 1. The ﬁve steps of the annotation process for VidLNs. We are able to cleanly describe even complex interactions and events by disentangling the storylines of different actors.\n",
      "\n",
      "to our\n",
      "\n",
      "location-output questions are the VideoQA datasets that ground parts of their textual ques- tions and answers (e.g., noun phrases) with bounding boxes on the video [8, 27, 62]. Closest to ours is TVQA+ [8], where the task is to choose one answer out of ﬁve and ground it spatially and temporally, on videos from a single TV show. In contrast, our answers can refer to any object,\n",
      "\n",
      "Related to our\n",
      "\n",
      "3\n",
      "\n",
      "Dataset\n",
      "\n",
      "#Videos\n",
      "\n",
      "#Narratives\n",
      "\n",
      "#Actors (per narr.)\n",
      "\n",
      "#Words (per narr.)\n",
      "\n",
      "Domain\n",
      "\n",
      "Grounding\n",
      "\n",
      "OVIS-VidLN UVO-VidLN Oops-VidLN\n",
      "\n",
      "607 7,588 12,128\n",
      "\n",
      "610 8,587 12,894\n",
      "\n",
      "1,799 (2.95) 25,755 (3.00) 44,422 (3.45)\n",
      "\n",
      "28,676 (47.01) 549,303 (63.97) 1,080,211 (83.78)\n",
      "\n",
      "General General General\n",
      "\n",
      "Every word Every word Every word\n",
      "\n",
      "VidLN All\n",
      "\n",
      "20,323\n",
      "\n",
      "22,091\n",
      "\n",
      "71,976 (3.54)\n",
      "\n",
      "1,658,190 (75.06)\n",
      "\n",
      "General\n",
      "\n",
      "Every word\n",
      "\n",
      "ActivityNet-Ent [65] MPII-MD [44] YouCook2-BB [66]\n",
      "\n",
      "14,926 94 667\n",
      "\n",
      "14,926 94 667\n",
      "\n",
      "- -\n",
      "\n",
      "General 647,814 (6,891.7) Movies ∼40,500 (60.72) Cooking\n",
      "\n",
      "745,876 (49.97)\n",
      "\n",
      "432 nouns People names 67 objects\n",
      "\n",
      "In Fig. 4, we compare the per-narrative counts for the 5 main word types to those of ActivityNet-Entities [65], the most related video dataset with grounded captions. On aver- age, our narratives have 75.1 words (including 23.0 nouns, 9.5 verbs, 8.5 adjectives, 7.2 adpositions, 2.4 pronouns). Thus our narratives are longer, and contain more words of each type, than those in ActivityNet-Entities. In summary, we present the largest and richest collection of general- domain video datasets with grounded captions to date. Semantic Accuracy. We evaluate manually how well the verbs and noun phrases of VidLN describe the actions and objects in the video. We randomly picked 70 videos in each dataset and checked for every verb and noun phrase whether the described object/action is present in the video, and if it is correctly described. Tab. 2 shows the results, demonstrating that the captions are almost perfectly semantically accurate. Localization Accuracy\n",
      "\n",
      "Figure 2. An example VidLN annotation. Each row shows the story of the video from the perspective of a different actor. Each mouse trace segment drawn on selected key-frames localizes the word highlighted in the same color. More examples are in Sec. S1.\n",
      "\n",
      "an input narrative with a segmentation mask on the video frames. This is a complex task that goes beyond open- vocabulary semantic segmentation [14], as often the text has multiple identical nouns that need to be disambiguated us- ing the context provided by other words. We construct two VNG benchmarks on OVIS and UVO, with a total of 8k videos involving 45k objects. We also build baseline mod- els that explicitly attempt this disambiguation and report ex- periments showing it’s beneﬁcial on this dataset, while also highlighting these new benchmarks are far from solved.\n",
      "\n",
      "5\n",
      "\n",
      "Accuracy (%)\n",
      "\n",
      "OVIS-VLN\n",
      "\n",
      "UVO-VLN\n",
      "\n",
      "Oops-VLN\n",
      "\n",
      "Noun phrases Verbs\n",
      "\n",
      "97.7 96.0\n",
      "\n",
      "96.8 97.8\n",
      "\n",
      "97.2 97.9\n",
      "\n",
      "Table 2. Semantic accuracy for noun phrases and verbs of our three VidLN datasets.\n",
      "\n",
      "shared with us). For our VidLNs, the precision measured on bounding boxes is 83.1% (73.2% measured on segmenta- tion masks). This means that our improvements in the anno- tation protocol have been very successful, and that VidLN traces have much higher quality than ImLN traces (+25%).\n",
      "\n",
      "4. Video Narrative Grounding (VNG)\n",
      "\n",
      "[67] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional videos. In American Ass. of Art. Intelligence, 2018. 3\n",
      "\n",
      "11\n",
      "\n",
      "Appendix\n",
      "\n",
      "S1. More Video Localized Narratives Exam-\n",
      "\n",
      "ples\n",
      "\n",
      "In Figs. S1 to S3, we show more examples of our VidLN\n",
      "\n",
      "annotations.\n",
      "\n",
      "S2. ReferFormer-VNG\n",
      "\n",
      "Here we describe implementation details and additional\n",
      "\n",
      "experiments for ReferFormer-VNG.\n",
      "\n",
      "[31] (“VideoSwin”), for most setups also achieves stronger results than the ResNet-50 backbone. However, when ini- tializing from ImageNet-1k and not training on UVO-VNG, then no video data (or only for OVIS-VNG ﬁne-tuning) is used. In this case, the Video Swin Transformer which is speciﬁcally designed for videos, cannot unfold its full potential and yields relatively weak results. When initializ- ing from Kinetics, and using UVO-VNG for main-training with videos, the VideoSwin results are strong, close to the results of Swin-Large.\n",
      "\n",
      "Using instead a Video Swin Transformer\n",
      "=============\n",
      "\n",
      "Question: What are video localized narratives?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query: What are video localized narratives?\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #0\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Abstract  We propose Video Localized Narratives, a new form of multimodal video annotations connecting vision and lan-\n",
      "guage. In the original Localized Narratives [40], annota- tors speak and move their mouse simultaneously on an im- age,\n",
      "thus grounding each word with a mouse trace segment. However, this is challenging on a video. Our new protocol empowers\n",
      "annotators to tell the story of a video with Local- ized Narratives, capturing even complex events involving multiple\n",
      "actors interacting with each other and with sev- eral passive objects. We annotated 20k videos of the OVIS, UVO, and\n",
      "Oops datasets, totalling 1.7M words. Based on this data, we also construct new benchmarks for the video narrative\n",
      "grounding and video question answering tasks, and provide reference results from strong baseline models. Our annotations\n",
      "are available at https://google. github.io/video-localized-narratives/.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #1\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Vision and language is a very active research area, which experienced much exciting progress recently [1, 32, 42, 43,\n",
      "52]. At the heart of many developments lie datasets con- necting still images to captions, while grounding some of the\n",
      "words in the caption to regions in the image, made with a variety of protocols [6,21,24,26,33,38,40,48,50,60]. The\n",
      "recent Localized Narratives (ImLNs) [40] offer a particu- larly attractive solution: the annotators describe an image\n",
      "with their voice while simultaneously moving their mouse over the regions they are describing. Speaking is natural and\n",
      "efﬁcient, resulting in long captions that describe the whole scene. Moreover, the synchronization between the voice and\n",
      "mouse pointer yields dense visual grounding for every word. Yet, still images only show one instant in time. Anno-\n",
      "tating videos would be even more interesting, as they show entire stories, featuring a ﬂow of events involving multiple\n",
      "actors and objects interacting with each other.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #2\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "3 2 0 2  r a  M 5 1  ]  V C . s c [  2 v 7 1 2 1 1 . 2 0 3 2 : v i X r a  Connecting Vision and Language with Video\n",
      "Localized Narratives  Paul Voigtlaender  Soravit Changpinyo  Jordi Pont-Tuset Radu Soricut Vittorio Ferrari  Google\n",
      "Research {voigtlaender,schangpi,jponttuset,rsoricut,vittoferrari}@google.com  Abstract\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #3\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "We annotated the OVIS [41], UVO [51], and Oops [12] datasets with Video Localized Narratives (VidLNs). These datasets\n",
      "cover a general domain, as opposed to only cook- ing [10, 66] or ﬁrst-person videos [16]. Moreover, these videos contain\n",
      "complex scenes with interactions between multiple actors and passive objects, leading to interesting stories that are\n",
      "captured by our rich annotations. In to- tal our annotations span 20k videos, 72k actors, and 1.7 million words. On\n",
      "average, each narrative features tran- scriptions for 3.5 actors, totaling 75.1 words, including 23.0 nouns, 9.5 verbs,\n",
      "and 8.5 adjectives (Fig. 4). Our analysis demonstrates the high quality of the data. The text descrip- tions almost\n",
      "always mention objects/actions that are actu- ally present in the video, and the mouse traces do lie inside their\n",
      "corresponding object in most cases.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #4\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Directly extending ImLNs to video by letting the anno- tator move their mouse and talk while the video is playing would\n",
      "lead to a “race against time”, likely resulting in fol- lowing only one salient object. In this paper, we propose  The\n",
      "richness of our VidLNs makes them a strong ba- sis for several tasks. We demonstrate this by creating new benchmarks for\n",
      "the tasks of Video Narrative Grounding (VNG) and Video Question Answering (VideoQA). The new VNG task requires a method\n",
      "to localize the nouns in  1  1. Watch2./3. Select actors and key-frames5. Transcription4. Speak and move mouse\n",
      "Background  Figure 1. The ﬁve steps of the annotation process for VidLNs. We are able to cleanly describe even complex\n",
      "interactions and events by disentangling the storylines of different actors.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #5\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "to our  location-output questions are the VideoQA datasets that ground parts of their textual ques- tions and answers\n",
      "(e.g., noun phrases) with bounding boxes on the video [8, 27, 62]. Closest to ours is TVQA+ [8], where the task is to\n",
      "choose one answer out of ﬁve and ground it spatially and temporally, on videos from a single TV show. In contrast, our\n",
      "answers can refer to any object,  Related to our  3  Dataset  #Videos  #Narratives  #Actors (per narr.)  #Words (per\n",
      "narr.)  Domain  Grounding  OVIS-VidLN UVO-VidLN Oops-VidLN  607 7,588 12,128  610 8,587 12,894  1,799 (2.95) 25,755\n",
      "(3.00) 44,422 (3.45)  28,676 (47.01) 549,303 (63.97) 1,080,211 (83.78)  General General General  Every word Every word\n",
      "Every word  VidLN All  20,323  22,091  71,976 (3.54)  1,658,190 (75.06)  General  Every word  ActivityNet-Ent [65] MPII-\n",
      "MD [44] YouCook2-BB [66]  14,926 94 667  14,926 94 667  - -  General 647,814 (6,891.7) Movies ∼40,500 (60.72) Cooking\n",
      "745,876 (49.97)  432 nouns People names 67 objects\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #6\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "In Fig. 4, we compare the per-narrative counts for the 5 main word types to those of ActivityNet-Entities [65], the most\n",
      "related video dataset with grounded captions. On aver- age, our narratives have 75.1 words (including 23.0 nouns, 9.5\n",
      "verbs, 8.5 adjectives, 7.2 adpositions, 2.4 pronouns). Thus our narratives are longer, and contain more words of each\n",
      "type, than those in ActivityNet-Entities. In summary, we present the largest and richest collection of general- domain\n",
      "video datasets with grounded captions to date. Semantic Accuracy. We evaluate manually how well the verbs and noun\n",
      "phrases of VidLN describe the actions and objects in the video. We randomly picked 70 videos in each dataset and checked\n",
      "for every verb and noun phrase whether the described object/action is present in the video, and if it is correctly\n",
      "described. Tab. 2 shows the results, demonstrating that the captions are almost perfectly semantically accurate.\n",
      "Localization Accuracy\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #7\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Figure 2. An example VidLN annotation. Each row shows the story of the video from the perspective of a different actor.\n",
      "Each mouse trace segment drawn on selected key-frames localizes the word highlighted in the same color. More examples\n",
      "are in Sec. S1.  an input narrative with a segmentation mask on the video frames. This is a complex task that goes\n",
      "beyond open- vocabulary semantic segmentation [14], as often the text has multiple identical nouns that need to be\n",
      "disambiguated us- ing the context provided by other words. We construct two VNG benchmarks on OVIS and UVO, with a total\n",
      "of 8k videos involving 45k objects. We also build baseline mod- els that explicitly attempt this disambiguation and\n",
      "report ex- periments showing it’s beneﬁcial on this dataset, while also highlighting these new benchmarks are far from\n",
      "solved.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #8\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "5  Accuracy (%)  OVIS-VLN  UVO-VLN  Oops-VLN  Noun phrases Verbs  97.7 96.0  96.8 97.8  97.2 97.9  Table 2. Semantic\n",
      "accuracy for noun phrases and verbs of our three VidLN datasets.  shared with us). For our VidLNs, the precision\n",
      "measured on bounding boxes is 83.1% (73.2% measured on segmenta- tion masks). This means that our improvements in the\n",
      "anno- tation protocol have been very successful, and that VidLN traces have much higher quality than ImLN traces (+25%).\n",
      "4. Video Narrative Grounding (VNG)\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #9\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: connecting-vision-and-language-with-video-localized-narratives.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "[67] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional\n",
      "videos. In American Ass. of Art. Intelligence, 2018. 3  11  Appendix  S1. More Video Localized Narratives Exam-  ples\n",
      "In Figs. S1 to S3, we show more examples of our VidLN  annotations.  S2. ReferFormer-VNG  Here we describe\n",
      "implementation details and additional  experiments for ReferFormer-VNG.  [31] (“VideoSwin”), for most setups also\n",
      "achieves stronger results than the ResNet-50 backbone. However, when ini- tializing from ImageNet-1k and not training on\n",
      "UVO-VNG, then no video data (or only for OVIS-VNG ﬁne-tuning) is used. In this case, the Video Swin Transformer which is\n",
      "speciﬁcally designed for videos, cannot unfold its full potential and yields relatively weak results. When initializ-\n",
      "ing from Kinetics, and using UVO-VNG for main-training with videos, the VideoSwin results are strong, close to the\n",
      "results of Swin-Large.  Using instead a Video Swin Transformer\n",
      "................................................................................\n",
      "Response:  Video Localized Narratives (VidLNs) are a new form of multimodal video annotations that connect vision and language.\n",
      "They empower annotators to tell the story of a video with Localized Narratives, capturing even complex events involving\n",
      "multiple actors interacting with each other and with several passive objects.\n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "filters = {\n",
    "    \"namespace\": \"document_name\",\n",
    "    \"allow_list\": [\"detecting-news-headline-hallucinations-with-explanations.pdf\"],\n",
    "}\n",
    "ask(\"What are video localized narratives?\", filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Of9XWLw9C653",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
      "\n",
      "Question: What is annotator diversity?\n",
      "\n",
      "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
      "\n",
      "Do not try to make up an answer:\n",
      " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
      " - If the context is empty, just say \"I do not know the answer to that.\"\n",
      "\n",
      "=============\n",
      "According to the survey results, a significant majority of respon- dents (75%) considered diversity to be a somewhat to extremely influential factor (>=3 on the Likert scale) in the quality of their annotated datasets. However, despite the perceived importance of various individual attributes and characteristics of annotators, very few of these factors were utilised in the recruitment process (see table 2). For instance, only 4 out of 15 practitioners who considered gender to be a relevant criterion included it in their selection criteria. Furthermore, even when additional information on annotator char- acteristics was available, it was rarely utilised in the recruitment process (table 2, column ‘Available Information’). These findings suggest a potential disconnect between the perceived importance of diversity in annotation and actual recruitment practices.\n",
      "\n",
      "Trovato et al.\n",
      "\n",
      "of the data annotators pool influence the dataset quality for [their] task?” using a five point Likert scale of 1 being “not at all influential” to 5 being “extremely influential”, followed by a question on why they believed annotator diversity is influential to the extent that they specified.\n",
      "\n",
      "4.2 Approaches to Annotator Diversity Below, we capture the varied perspectives to annotator diversity among the participants in our study, from some who considered diversity as irrelevant, to others who made efforts to accommodate it, even if only in partial ways. Most practitioners acknowledged the role of annotator subjectivities in the annotation process; some emphasised the importance of diversity in achieving a balanced view and bringing previously overlooked sub-populations into con- sideration. However, many went on to explain their decisions to not consider annotator diversity in setting up annotation tasks. Their primary focus was on achieving a certain level of quality, which was often measured against how closely an annotator followed pre- defined parameters and guidelines. We discuss the justifications provided by practitioners for taking a representationalist approach to annotator diversity and prioritising measures of quality.\n",
      "\n",
      "4.3 Barriers to Incorporating Diversity In our study, participants identified several challenges to accommo- dating annotator diversity. Firstly, participants reported a lack of access to information about annotators, hindering their ability to understand and account for annotators’ unique perspectives and backgrounds. Additionally, the limited communication and collabo- ration between practitioners and annotators resulted in practition- ers having minimal knowledge of annotators beyond their worker identification (worker-ID). Lastly, the lack of clear and actionable pathways to incorporate annotator socio-demographic information into the development and evaluation process further diminished\n",
      "\n",
      "Trovato et al.\n",
      "\n",
      "the motivation for practitioners to prioritise diversity among data annotators.\n",
      "\n",
      ".g., which regions or individual identities are covered) and the effects of accounting for such diversity on the performance of downstream ML models [85]. Researchers have argued that collecting more diverse data for rele- vant dimensions could lead to fairer ML models [8]. Fazelpour and De-Arteaga [28] emphasised the ways in which diversity-related considerations are embedded throughout the design of ML systems, regardless of whether they are actively recognised [28]. We con- tribute to this area of research by focusing on the conceptions of diversity within data annotator pools.\n",
      "\n",
      ". Each of these questions had socio-demographic attributes such as the annotators’ age, gender; expertise; location; and more. We also ask, “in [their] experience, to what extent does the diversity\n",
      "\n",
      "Analysis. We computed a range of descriptive statistics using SPSS to better understand practitioners’ approaches to diversity, and particularly the kinds of diversity they view as relevant, if at all. These included descriptives to questions presented with Likert scale response options and multiple choice answers (e.g., the chal- lenges in recruiting the desired pool of annotators). We focused on comparing the differences between the attributes which were used to recruit annotators, the kinds of information available to practitioners and what they would see as relevant attributes in the ideal scenario. In cases where questions were completed by a subset of the respondents, we report question-specific response rates and percentage of respondents who answered that question. Finally, we conducted a qualitative analysis to the open-ended questions following the same to the interviews (see the following section 3.2 below)\n",
      "\n",
      "Additionally, our interviews revealed significant contrast be- tween participants’ reflections on the concept of annotator diver- sity and its actual implementation. Annotator diversity was often understood as representative of a particular perspective or point of view. For example, annotators were selected based on the low- resource dialect they spoke or the high flood risk areas they lived in, using language and location as proxies for diversity. Very few participants actively recruited annotators based on their lived ex- periences, knowledge, or expertise as facets of diversity. In the rare cases where local knowledge and expertise were considered im- portant, measurable criteria were applied to assess the annotators’\n",
      "\n",
      "Trovato et al.\n",
      "\n",
      "Criteria\n",
      "\n",
      "Relevant attributes\n",
      "\n",
      "Selection criteria\n",
      "\n",
      "Available information\n",
      "\n",
      "n = 11\n",
      "\n",
      "n = 2\n",
      "\n",
      "n = 1\n",
      "\n",
      "n = 2 n = 5\n",
      "\n",
      "Table 2: The number of survey respondents for three questions: One, the at- tributes of data workers which could affect the annotations; Two, respon- dents’ criteria for selecting annotators, and third, the kinds of information available to practitioners about the annotators of their dataset. Each ques- tion had attributes such as the annotators’ age, gender, expertise, location, political orientation, and more.\n",
      "\n",
      "expertise or knowledge. For instance, P6’s motivation for select- ing an annotator for a mapping project was to include individuals from underrepresented backgrounds and “capture the other parts of the population”. Similarly, P9 spoke of recruiting “a person of X identity with Y knowledge” to ensure diversity. Overall, interview participants demonstrated a view of diversity through the lens of categories and metrics, rather than tied to experiential knowledge and expertise.\n",
      "\n",
      "3 METHODOLOGY Previous human-centered investigations concerning annotators’ subjectivities, biases, and efficiency have primarily focused on an- notators’ perspectives [24, 63]. In this research, we focused on AI practitioners in order to understand their conceptions of annotator diversity [68]. To gain a holistic view of data annotation practices, we employed a mixed-methods approach with a sequential explana- tory design [45]. This involved conducting a survey with the aim of eliciting broad-brushed and higher-level perspectives from a wide audience. The survey was followed by semi-structured interviews to investigate, in more detail, the ways in which practitioners apply their understanding of annotator diversity in practice. The qualita- tive data was used to elaborate and explain the survey results (e.g., the rationale behind data annotation task design), and served as the foundation for our inquiry\n",
      "=============\n",
      "\n",
      "Question: What is annotator diversity?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query: What is annotator diversity?\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #0\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: annotator-diversity-in-data-practices.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "According to the survey results, a significant majority of respon- dents (75%) considered diversity to be a somewhat to\n",
      "extremely influential factor (>=3 on the Likert scale) in the quality of their annotated datasets. However, despite the\n",
      "perceived importance of various individual attributes and characteristics of annotators, very few of these factors were\n",
      "utilised in the recruitment process (see table 2). For instance, only 4 out of 15 practitioners who considered gender to\n",
      "be a relevant criterion included it in their selection criteria. Furthermore, even when additional information on\n",
      "annotator char- acteristics was available, it was rarely utilised in the recruitment process (table 2, column ‘Available\n",
      "Information’). These findings suggest a potential disconnect between the perceived importance of diversity in annotation\n",
      "and actual recruitment practices.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #1\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: annotator-diversity-in-data-practices.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Trovato et al.  of the data annotators pool influence the dataset quality for [their] task?” using a five point Likert\n",
      "scale of 1 being “not at all influential” to 5 being “extremely influential”, followed by a question on why they\n",
      "believed annotator diversity is influential to the extent that they specified.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #2\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: annotator-diversity-in-data-practices.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "4.2 Approaches to Annotator Diversity Below, we capture the varied perspectives to annotator diversity among the\n",
      "participants in our study, from some who considered diversity as irrelevant, to others who made efforts to accommodate\n",
      "it, even if only in partial ways. Most practitioners acknowledged the role of annotator subjectivities in the annotation\n",
      "process; some emphasised the importance of diversity in achieving a balanced view and bringing previously overlooked\n",
      "sub-populations into con- sideration. However, many went on to explain their decisions to not consider annotator\n",
      "diversity in setting up annotation tasks. Their primary focus was on achieving a certain level of quality, which was\n",
      "often measured against how closely an annotator followed pre- defined parameters and guidelines. We discuss the\n",
      "justifications provided by practitioners for taking a representationalist approach to annotator diversity and\n",
      "prioritising measures of quality.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #3\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: annotator-diversity-in-data-practices.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "4.3 Barriers to Incorporating Diversity In our study, participants identified several challenges to accommo- dating\n",
      "annotator diversity. Firstly, participants reported a lack of access to information about annotators, hindering their\n",
      "ability to understand and account for annotators’ unique perspectives and backgrounds. Additionally, the limited\n",
      "communication and collabo- ration between practitioners and annotators resulted in practition- ers having minimal\n",
      "knowledge of annotators beyond their worker identification (worker-ID). Lastly, the lack of clear and actionable\n",
      "pathways to incorporate annotator socio-demographic information into the development and evaluation process further\n",
      "diminished  Trovato et al.  the motivation for practitioners to prioritise diversity among data annotators.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #4\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: annotator-diversity-in-data-practices.pdf\n",
      "................................................................................\n",
      "Content: \n",
      ".g., which regions or individual identities are covered) and the effects of accounting for such diversity on the\n",
      "performance of downstream ML models [85]. Researchers have argued that collecting more diverse data for rele- vant\n",
      "dimensions could lead to fairer ML models [8]. Fazelpour and De-Arteaga [28] emphasised the ways in which diversity-\n",
      "related considerations are embedded throughout the design of ML systems, regardless of whether they are actively\n",
      "recognised [28]. We con- tribute to this area of research by focusing on the conceptions of diversity within data\n",
      "annotator pools.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #5\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: annotator-diversity-in-data-practices.pdf\n",
      "................................................................................\n",
      "Content: \n",
      ". Each of these questions had socio-demographic attributes such as the annotators’ age, gender; expertise; location; and\n",
      "more. We also ask, “in [their] experience, to what extent does the diversity\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #6\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: annotator-diversity-in-data-practices.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Analysis. We computed a range of descriptive statistics using SPSS to better understand practitioners’ approaches to\n",
      "diversity, and particularly the kinds of diversity they view as relevant, if at all. These included descriptives to\n",
      "questions presented with Likert scale response options and multiple choice answers (e.g., the chal- lenges in recruiting\n",
      "the desired pool of annotators). We focused on comparing the differences between the attributes which were used to\n",
      "recruit annotators, the kinds of information available to practitioners and what they would see as relevant attributes\n",
      "in the ideal scenario. In cases where questions were completed by a subset of the respondents, we report question-\n",
      "specific response rates and percentage of respondents who answered that question. Finally, we conducted a qualitative\n",
      "analysis to the open-ended questions following the same to the interviews (see the following section 3.2 below)\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #7\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: annotator-diversity-in-data-practices.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Additionally, our interviews revealed significant contrast be- tween participants’ reflections on the concept of\n",
      "annotator diver- sity and its actual implementation. Annotator diversity was often understood as representative of a\n",
      "particular perspective or point of view. For example, annotators were selected based on the low- resource dialect they\n",
      "spoke or the high flood risk areas they lived in, using language and location as proxies for diversity. Very few\n",
      "participants actively recruited annotators based on their lived ex- periences, knowledge, or expertise as facets of\n",
      "diversity. In the rare cases where local knowledge and expertise were considered im- portant, measurable criteria were\n",
      "applied to assess the annotators’  Trovato et al.  Criteria  Relevant attributes  Selection criteria  Available\n",
      "information\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #8\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: annotator-diversity-in-data-practices.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "n = 11  n = 2  n = 1  n = 2 n = 5  Table 2: The number of survey respondents for three questions: One, the at- tributes\n",
      "of data workers which could affect the annotations; Two, respon- dents’ criteria for selecting annotators, and third,\n",
      "the kinds of information available to practitioners about the annotators of their dataset. Each ques- tion had\n",
      "attributes such as the annotators’ age, gender, expertise, location, political orientation, and more.  expertise or\n",
      "knowledge. For instance, P6’s motivation for select- ing an annotator for a mapping project was to include individuals\n",
      "from underrepresented backgrounds and “capture the other parts of the population”. Similarly, P9 spoke of recruiting “a\n",
      "person of X identity with Y knowledge” to ensure diversity. Overall, interview participants demonstrated a view of\n",
      "diversity through the lens of categories and metrics, rather than tied to experiential knowledge and expertise.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #9\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: annotator-diversity-in-data-practices.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "3 METHODOLOGY Previous human-centered investigations concerning annotators’ subjectivities, biases, and efficiency have\n",
      "primarily focused on an- notators’ perspectives [24, 63]. In this research, we focused on AI practitioners in order to\n",
      "understand their conceptions of annotator diversity [68]. To gain a holistic view of data annotation practices, we\n",
      "employed a mixed-methods approach with a sequential explana- tory design [45]. This involved conducting a survey with\n",
      "the aim of eliciting broad-brushed and higher-level perspectives from a wide audience. The survey was followed by semi-\n",
      "structured interviews to investigate, in more detail, the ways in which practitioners apply their understanding of\n",
      "annotator diversity in practice. The qualita- tive data was used to elaborate and explain the survey results (e.g., the\n",
      "rationale behind data annotation task design), and served as the foundation for our inquiry\n",
      "................................................................................\n",
      "Response:  Annotator diversity refers to the variety of backgrounds, experiences, and perspectives represented among the\n",
      "individuals who annotate data. It encompasses various attributes such as age, gender, expertise, location, language, and\n",
      "cultural background. Annotator diversity is considered important because it helps ensure a balanced view, brings\n",
      "previously overlooked sub-populations into consideration, and reduces biases in the annotated data.\n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "ask(\"What is annotator diversity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "jLTasbssQfdG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
      "\n",
      "Question: What are challenges in creating a realistic simulation of city traffic?\n",
      "\n",
      "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
      "\n",
      "Do not try to make up an answer:\n",
      " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
      " - If the context is empty, just say \"I do not know the answer to that.\"\n",
      "\n",
      "=============\n",
      ". Unfortunately, there is no reproducible process to create accurate demand data as of now, which is where most of the major challenges of realistic traffic simulation remain. However, demand data accuracy can still be slightly improved through calibration against ground data (Section 7).\n",
      "\n",
      "CALIBRATION The challenges of creating a realistic microsimulation of city traffic lie in its calibration. To cal- ibrate a microsimulation, one first needs to fix all the network issues (Section 7.1). Then, OD demand can be calibrated without simulation by matching the total counts of vehicles entering or exiting the network (Section 7.2) followed by calibration with macrosimulation by matching all detector counts in the network (Section 7.3). Once the OD demand is calibrated in this manner and after having chosen a route choice model (Section 7.4), the driving behaviors are then calibrated (Section 7.5) using a genetic algorithm (Section 7.6). The OD demand calibration and driving behavior calibration procedures can be reproduced from the provided open-source code4.\n",
      "\n",
      "Before calibrating a microsimulation (Section 7), one needs first to fix any existing network and demand issues (connectivity issues, wrong number of lanes, wrong traffic signal plans, small mistakes in the master traffic control plan, obvious error in the demand data). The first phase of calibration is done without simulation by matching simulated and ground total counts of vehicles entering or exiting the network. This is then followed up by the second phase of calibration, done through macrosimulation. Once the OD demand is calibrated, the driving behaviors (routing, car- following, lane-changing models), and microsimulation parameters (like simulation time step) can be calibrated using optimization algorithms that work with expensive function evaluations (this work uses a genetic algorithm that is highly parallelizable).\n",
      "\n",
      "Updating incorrect lane connections at intersections. • Changing erroneous road section geometries. • Changing wrong numbers of lanes on road sections. • Removing parking lots, where cut-throughs were performed in the simulation to avoid congestion at traffic lights.\n",
      "\n",
      "Updating improperly imported traffic signal plans. • Updating master traffic control plans to solve missing synchronization between traffic signal plans. The authors found that there was one ground truth congestion issue that could be solved by synchronizing traffic lights operated by the state with the traffic lights operated by the city on Auto-Mall Pkwy and I-680.\n",
      "\n",
      "The authors realized during this process that the input demand data used was biased towards the northwest, which is likely a byproduct of the demand being sourced from the SFCTA CHAMP demand model, which aims to replicate traffic in San Francisco (which is to the northwest of Fremont). An accurate initial demand data is key for a realistic simulation.\n",
      "\n",
      "Creating, Calibrating, and Validating Large-Scale Microscopic Traffic Simulation\n",
      "\n",
      "Theophile Cabannes1 Google Research cabannes@google.com\n",
      "\n",
      "Alben Rome Bagabaldo Department of Civil and Environmental Engineering University of California, Berkeley, CA 94720, USA bagabaldo@berkeley.edu\n",
      "\n",
      "Junghwan Lee2 Department of Electrical Engineering and Computer Sciences University of California, Berkeley, CA 94720, USA johnjhlee@berkeley.edu\n",
      "\n",
      "Qianxin Gan Department of Electrical Engineering and Computer Sciences University of California, Berkeley, CA 94720, USA ganqx2001@berkeley.edu\n",
      "\n",
      "Ayush Jain1 Google ayush.jain@berkeley.edu\n",
      "\n",
      "Alice Blondel École Nationale Supérieure de Techniques Avancées Paris aliceblondel99@gmail.com\n",
      "\n",
      "Alexandre Bayen3 Department of Electrical Engineering and Computer Sciences Department of Civil and Environmental Engineering University of California, Berkeley, CA 94720, USA bayen@berkeley.edu\n",
      "\n",
      "Published: January, 2023\n",
      "\n",
      "Running a simulation To run a traffic simulation in Aimsun, one first needs to create/import the network. Second, the OD demand data should be imported, and a traffic demand (a list of timed OD demand matrices with scaling factors) should be generated. Optionally, traffic data can also be imported. Once all imports are complete, a traffic simulation can be created, with many mutable parameters (see full list in Section 7.5). From here, the simulation can be run.\n",
      "\n",
      "Creating and running a simulation can be done using the Aimsun’s GUI, but, for the sake of reproducibility, the authors opted to write Python scripts for each step of the process4. The open-source repository is self-contained, and any readers with an Aimsun 22.0.1 license should be able to reproduce all the steps explained below and run the same simulation performed by the authors.\n",
      "\n",
      ". 39–65. Shafiei, S., Z. Gu, and M. Saberi, Calibration and validation of a simulation-based dy- namic traffic assignment model for a large-scale congested network. Simulation Modelling Practice and Theory, Vol. 86, 2018, pp. 169–186. Chao, Q., H. Bi, W. Li, T. Mao, Z. Wang, M. C. Lin, and Z. Deng, A Survey on Visual Traf- fic Simulation: Models, Evaluations, and Applications in Autonomous Driving. Computer Graphics Forum, Vol. 39, No. 1, 2020, pp. 287–308. Persula, M., Simulation of Traffic Systems - An Overview. Journal of Geographic Infor- mation and Decision Analysis, Vol. 3, No. 1, 1999, pp. 1–8. Antoniou, C., J. Barcelo, M. Brackstone, H. B. Celikoglu, B. Ciuffo, V. Punzo, P. Sykes, T. Toledo, P. Vortisch, and P. Wagner, Fundamentals of Traffic Simulation, Vol. 145 of International Series in Operations Research & Management Science. Springer New York, New York, NY, 2010. Lieberman, E. and A. K. Rathi, Traffic simulation. Federal Highway Administration, 1992\n",
      "\n",
      "POST-PROCESSING ANALYSIS After the simulation is created, one needs to validate the accuracy of the simulation to ensure the credibility of its results. Once the accuracy of the simulation is satisfactory, traffic analysis can be conducted to observe how certain metrics change in different scenarios.\n",
      "\n",
      "Validation The first step in validating the simulation is to use Measures of Effectiveness (MOE) (13, 60), which serve as indicators for the general correctness of system-wide results. Some examples of metrics that can be used for MOE are average delay time, total distance traveled by vehicles in the entire network, and the average number of vehicles in the virtual queue at each time step.\n",
      "\n",
      "Organization This article first gives an overview of the different type of simulations a city can perform (Section 3. It describes the process to create, calibrate and validate traffic microsimulations (Section 4). Then, the article specifies the input data needed (Section 5) and the steps to follow (Section 6) to run an Aimsun simulation. Finally, it describes the calibration process (Section 7) and the output analysis that can be done after running the simulation (Section 8). Each section is accompanied by references to the open-source code repository4 such that the reader can replicate the processes discussed in this article. In addition, data and results from Fremont, California, USA, are included to provide motivating examples for the reader.\n",
      "\n",
      "EXISTING TRAFFIC SIMULATOR To understand the type of simulations that cities can use, it is important to understand how traffic is shaped by cities.\n",
      "\n",
      "4https://github.com/Fremont-project/traffic-microsimulation\n",
      "\n",
      "5\n",
      "\n",
      "Traffic models and simulators Assigning the OD demand to dynamic routes is done by dynamic traffic assignment models (32, 33). Once the route of each vehicle is known, computing the dynamic road section traffic loads in the network is called dynamic network loading (32).\n",
      "\n",
      "In this dissertation, we refer to the traffic demand as the demand that travels using the road network, without a route being specified. However, in the transportation engineering academic lit- erature, the traffic demand might refer to a different notion: in the four-step model (26), it includes the route assignment. Our choice is motivated by the fact that we are interested in understanding\n",
      "\n",
      "6\n",
      "\n",
      "Cabannes, T., Bagabaldo A.R., et al.\n",
      "=============\n",
      "\n",
      "Question: What are challenges in creating a realistic simulation of city traffic?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query: What are challenges in creating a realistic simulation of city traffic?\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #0\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf\n",
      "................................................................................\n",
      "Content: \n",
      ". Unfortunately, there is no reproducible process to create accurate demand data as of now, which is where most of the\n",
      "major challenges of realistic traffic simulation remain. However, demand data accuracy can still be slightly improved\n",
      "through calibration against ground data (Section 7).\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #1\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "CALIBRATION The challenges of creating a realistic microsimulation of city traffic lie in its calibration. To cal-\n",
      "ibrate a microsimulation, one first needs to fix all the network issues (Section 7.1). Then, OD demand can be calibrated\n",
      "without simulation by matching the total counts of vehicles entering or exiting the network (Section 7.2) followed by\n",
      "calibration with macrosimulation by matching all detector counts in the network (Section 7.3). Once the OD demand is\n",
      "calibrated in this manner and after having chosen a route choice model (Section 7.4), the driving behaviors are then\n",
      "calibrated (Section 7.5) using a genetic algorithm (Section 7.6). The OD demand calibration and driving behavior\n",
      "calibration procedures can be reproduced from the provided open-source code4.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #2\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Before calibrating a microsimulation (Section 7), one needs first to fix any existing network and demand issues\n",
      "(connectivity issues, wrong number of lanes, wrong traffic signal plans, small mistakes in the master traffic control\n",
      "plan, obvious error in the demand data). The first phase of calibration is done without simulation by matching simulated\n",
      "and ground total counts of vehicles entering or exiting the network. This is then followed up by the second phase of\n",
      "calibration, done through macrosimulation. Once the OD demand is calibrated, the driving behaviors (routing, car-\n",
      "following, lane-changing models), and microsimulation parameters (like simulation time step) can be calibrated using\n",
      "optimization algorithms that work with expensive function evaluations (this work uses a genetic algorithm that is highly\n",
      "parallelizable).\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #3\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Updating incorrect lane connections at intersections. • Changing erroneous road section geometries. • Changing wrong\n",
      "numbers of lanes on road sections. • Removing parking lots, where cut-throughs were performed in the simulation to avoid\n",
      "congestion at traffic lights.  Updating improperly imported traffic signal plans. • Updating master traffic control\n",
      "plans to solve missing synchronization between traffic signal plans. The authors found that there was one ground truth\n",
      "congestion issue that could be solved by synchronizing traffic lights operated by the state with the traffic lights\n",
      "operated by the city on Auto-Mall Pkwy and I-680.  The authors realized during this process that the input demand data\n",
      "used was biased towards the northwest, which is likely a byproduct of the demand being sourced from the SFCTA CHAMP\n",
      "demand model, which aims to replicate traffic in San Francisco (which is to the northwest of Fremont). An accurate\n",
      "initial demand data is key for a realistic simulation.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #4\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Creating, Calibrating, and Validating Large-Scale Microscopic Traffic Simulation  Theophile Cabannes1 Google Research\n",
      "cabannes@google.com  Alben Rome Bagabaldo Department of Civil and Environmental Engineering University of California,\n",
      "Berkeley, CA 94720, USA bagabaldo@berkeley.edu  Junghwan Lee2 Department of Electrical Engineering and Computer Sciences\n",
      "University of California, Berkeley, CA 94720, USA johnjhlee@berkeley.edu  Qianxin Gan Department of Electrical\n",
      "Engineering and Computer Sciences University of California, Berkeley, CA 94720, USA ganqx2001@berkeley.edu  Ayush Jain1\n",
      "Google ayush.jain@berkeley.edu  Alice Blondel École Nationale Supérieure de Techniques Avancées Paris\n",
      "aliceblondel99@gmail.com  Alexandre Bayen3 Department of Electrical Engineering and Computer Sciences Department of\n",
      "Civil and Environmental Engineering University of California, Berkeley, CA 94720, USA bayen@berkeley.edu  Published:\n",
      "January, 2023\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #5\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Running a simulation To run a traffic simulation in Aimsun, one first needs to create/import the network. Second, the OD\n",
      "demand data should be imported, and a traffic demand (a list of timed OD demand matrices with scaling factors) should be\n",
      "generated. Optionally, traffic data can also be imported. Once all imports are complete, a traffic simulation can be\n",
      "created, with many mutable parameters (see full list in Section 7.5). From here, the simulation can be run.  Creating\n",
      "and running a simulation can be done using the Aimsun’s GUI, but, for the sake of reproducibility, the authors opted to\n",
      "write Python scripts for each step of the process4. The open-source repository is self-contained, and any readers with\n",
      "an Aimsun 22.0.1 license should be able to reproduce all the steps explained below and run the same simulation performed\n",
      "by the authors.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #6\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf\n",
      "................................................................................\n",
      "Content: \n",
      ". 39–65. Shafiei, S., Z. Gu, and M. Saberi, Calibration and validation of a simulation-based dy- namic traffic\n",
      "assignment model for a large-scale congested network. Simulation Modelling Practice and Theory, Vol. 86, 2018, pp.\n",
      "169–186. Chao, Q., H. Bi, W. Li, T. Mao, Z. Wang, M. C. Lin, and Z. Deng, A Survey on Visual Traf- fic Simulation:\n",
      "Models, Evaluations, and Applications in Autonomous Driving. Computer Graphics Forum, Vol. 39, No. 1, 2020, pp. 287–308.\n",
      "Persula, M., Simulation of Traffic Systems - An Overview. Journal of Geographic Infor- mation and Decision Analysis,\n",
      "Vol. 3, No. 1, 1999, pp. 1–8. Antoniou, C., J. Barcelo, M. Brackstone, H. B. Celikoglu, B. Ciuffo, V. Punzo, P. Sykes,\n",
      "T. Toledo, P. Vortisch, and P. Wagner, Fundamentals of Traffic Simulation, Vol. 145 of International Series in\n",
      "Operations Research & Management Science. Springer New York, New York, NY, 2010. Lieberman, E. and A. K. Rathi, Traffic\n",
      "simulation. Federal Highway Administration, 1992\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #7\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "POST-PROCESSING ANALYSIS After the simulation is created, one needs to validate the accuracy of the simulation to ensure\n",
      "the credibility of its results. Once the accuracy of the simulation is satisfactory, traffic analysis can be conducted\n",
      "to observe how certain metrics change in different scenarios.  Validation The first step in validating the simulation is\n",
      "to use Measures of Effectiveness (MOE) (13, 60), which serve as indicators for the general correctness of system-wide\n",
      "results. Some examples of metrics that can be used for MOE are average delay time, total distance traveled by vehicles\n",
      "in the entire network, and the average number of vehicles in the virtual queue at each time step.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #8\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Organization This article first gives an overview of the different type of simulations a city can perform (Section 3. It\n",
      "describes the process to create, calibrate and validate traffic microsimulations (Section 4). Then, the article\n",
      "specifies the input data needed (Section 5) and the steps to follow (Section 6) to run an Aimsun simulation. Finally, it\n",
      "describes the calibration process (Section 7) and the output analysis that can be done after running the simulation\n",
      "(Section 8). Each section is accompanied by references to the open-source code repository4 such that the reader can\n",
      "replicate the processes discussed in this article. In addition, data and results from Fremont, California, USA, are\n",
      "included to provide motivating examples for the reader.  EXISTING TRAFFIC SIMULATOR To understand the type of\n",
      "simulations that cities can use, it is important to understand how traffic is shaped by cities.\n",
      "4https://github.com/Fremont-project/traffic-microsimulation  5\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #9\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: creating-calibrating-and-validating-large-scale-microscopic-traffic-simulation.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Traffic models and simulators Assigning the OD demand to dynamic routes is done by dynamic traffic assignment models\n",
      "(32, 33). Once the route of each vehicle is known, computing the dynamic road section traffic loads in the network is\n",
      "called dynamic network loading (32).  In this dissertation, we refer to the traffic demand as the demand that travels\n",
      "using the road network, without a route being specified. However, in the transportation engineering academic lit-\n",
      "erature, the traffic demand might refer to a different notion: in the four-step model (26), it includes the route\n",
      "assignment. Our choice is motivated by the fact that we are interested in understanding  6  Cabannes, T., Bagabaldo\n",
      "A.R., et al.\n",
      "................................................................................\n",
      "Response:  The challenges of creating a realistic microsimulation of city traffic lie in its calibration. To calibrate a\n",
      "microsimulation, one first needs to fix all the network issues. Then, OD demand can be calibrated without simulation by\n",
      "matching the total counts of vehicles entering or exiting the network, followed by calibration with macrosimulation by\n",
      "matching all detector counts in the network. Once the OD demand is calibrated in this manner and after having chosen a\n",
      "route choice model, the driving behaviors are then calibrated using a genetic algorithm.\n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "ask(\"What are challenges in creating a realistic simulation of city traffic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ZSDaw13bqkvF",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
      "\n",
      "Question: What is standard contrastive learning?\n",
      "\n",
      "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
      "\n",
      "Do not try to make up an answer:\n",
      " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
      " - If the context is empty, just say \"I do not know the answer to that.\"\n",
      "\n",
      "=============\n",
      "There have been a number of other attempts to unify different contrastive learning techniques in a single theoretical framework. Balestriero & LeCun (2022) describe a uniﬁcation using the framework of spectral embedding methods, and draw connections between SimCLR and Kernel ISOMAP. Tian (2022) provides a game-theoretic uniﬁcation, and shows that contrastive losses are related to PCA in the input space for the special case of deep linear networks.\n",
      "\n",
      "Recall that during contrastive learning we have a distribution p(Z) of original examples, and a distribution p(A|Z = z) of augmented views for each original example z. Then, at downstream supervised learning time, we additionally have a distribution p(Y = y|Z = z) of labels y ∈ R for each original example z. (For simplicity we assume that the distribution of p(Z) remains unchanged during the downstream learning step. We also assume y is a scalar; the vector case can be derived by independently estimating each element of Y with a separate target function.)\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "When using a contrastive learning method such as SimCLR (Chen et al., 2020a) for representation learning, the ﬁrst step is to specify the distribution of original examples z ∼ p(Z) within some space Z along with a sampler of augmented views p(A|Z = z) over a potentially different space A. 1 For example, p(Z) might represent a dataset of natural images, and p(A|Z) a random transformation that applies random scaling and color shifts. Contrastive learning then consists of ﬁnding a parameterized mapping (such as a neural network) which maps multiple views of the same image (e.g. draws from a1, a2 ∼ p(A|Z = z) for a ﬁxed z) close together, and unrelated views far apart. This mapping can then be used to deﬁne a representation which is useful for downstream supervised learning.\n",
      "\n",
      "K+(a1, a2) =\n",
      "\n",
      "p+(a1, a2) p(a1)p(a2)\n",
      "\n",
      "= (cid:104)φ+(a1), φ+(a2)(cid:105) ,\n",
      "\n",
      "(1)\n",
      "\n",
      "where p+(a1, a2) = (cid:80)\n",
      "\n",
      "φ+(a) =\n",
      "\n",
      "z p(a1|z)p(a2|z)p(z), and φ+ : A → R|Z| is the transformation (cid:104) p(a|z1)\n",
      "\n",
      "√\n",
      "\n",
      "√\n",
      "\n",
      "√\n",
      "\n",
      "(cid:105)(cid:62)\n",
      "\n",
      "p(a|z|Z|)\n",
      "\n",
      "p(z|Z|)\n",
      "\n",
      "p(z1)\n",
      "\n",
      "p(a|z2)\n",
      "\n",
      "p(z2)\n",
      "\n",
      "· ·\n",
      "\n",
      "p(a)\n",
      "\n",
      "p(a)\n",
      "\n",
      "p(a)\n",
      "\n",
      ".\n",
      "\n",
      "(2)\n",
      "\n",
      "Here, the magnitude of the dot product between vectors φ+(a1) and φ+(a2) reﬂects the relative likelihood of data points a1 and a2 being drawn from a positive pair v.s. a negative pair. In particular, if a1 and a2 have zero probability of being a positive pair, φ+(a1) and φ+(a2) are orthogonal.\n",
      "\n",
      "As shown in Figure 1, we can thus reinterpret the three contrastive learning methods in Table 1 as kernel learning methods, in that they produce parameterized positive-deﬁnite kernel functions which approximate this positive-pair kernel. By investigating properties of this kernel, we can thus hope to build a better understanding of the behavior of contrastive learning.\n",
      "\n",
      "The success of these representations have led to a variety of theoretical analyses of contrastive learning, including analyses based on conditional independence within latent classes (Saunshi et al., 2019), alignment of hyperspherical embeddings (Wang & Isola, 2020), conditional independence structure with landmark embeddings (Tosh et al., 2021), and spectral analysis of an augmentation graph (HaoChen et al., 2021). Each of these analyses is based on a single choice of contrastive learning objective. In this work, we go further by integrating multiple popular contrastive learning methods into a single framework, and showing that it can be used to build minimax-optimal representations under a straightforward assumption about similarity of labels between positive pairs.\n",
      "\n",
      "2 ))2(cid:105)\n",
      "\n",
      "1 , a−\n",
      "\n",
      "Figure 1: The positive-pair ker- nel K+ assigns high similarity to likely positive pairs. Contrastive learning methods learn parame- terized kernels (cid:98)Kθ which assign high similarity to nearby points in a learned embedding space.\n",
      "\n",
      "Table 1: Existing contrastive learning objectives, reinterpreted as learning parameterized approximations of K+. “Minimum” de- notes the population minimum of the loss over all kernel functions (not necessarily representable using the shown parameterization). C[a1] is a equivalence-class-dependent proportionality constant, with C[a1] = C[a2] whenever p+(a1, a2) > 0. See Appendix B for derivations and discussion of other related objectives.\n",
      "\n",
      "We conclude by studying the behavior of contrastive learning models on two synthetic tasks for which the exact positive-pair kernel is known, and investigating the extent to which the basis of eigenfunctions can be extracted from trained models. As predicted by our theory, we ﬁnd that the same eigenfunctions can be recovered from multiple model parameterizations and losses, although the accuracy depends on both kernel parameterization expressiveness and augmentation strength.\n",
      "\n",
      "2 CONTRASTIVE LEARNING IS SECRETLY KERNEL LEARNING\n",
      "\n",
      "We note that applying a rotation matrix to the optimal representation does not affect the expressivity of that representation. It is thus sufﬁcient to identify the subspace spanned by the ﬁrst d principal component projection functions. If the representation dimension d is known in advance, it may be possible to adjust the contrastive learning method to accomplish this without requiring a separate PCA step. In particular, when using the spectral contrastive loss with a d-dimensional linear kernel head, the population minimizer of the loss will exactly correspond to the best d-dimensional approximation of K+. This means that the output layer representation will be exactly the set of principal component projection functions rotated by some orthogonal matrix.\n",
      "\n",
      "Indeed, such a minimizer is equivalent to K+ except that it scales the inner product by the value of b(a) for each communicating class. It is still possible to extract the set of Markov chain eigenfunctions from the set of principal components of this kernel, although one must correct for the scaling factor when computing the eigenvalues; see Appendix C.4 for details. Alternatively, one can ensure a unique minimum by combining the NT-Xent/InfoNCE loss with either the spectral or logistic losses (discussed below).\n",
      "\n",
      "B.2 LOGISTIC LOSSES AND NT-LOGISTIC\n",
      "\n",
      "Logistic losses have also been proposed for contrastive learning, including the NT-Logistic objective as described in Chen et al. (2020a) and other versions described by Mikolov et al. (2013) and Tosh\n",
      "\n",
      "16\n",
      "\n",
      "Published as a conference paper at ICLR 2023\n",
      "\n",
      "et al. (2021). Such losses take the form\n",
      "\n",
      "LLogistic(fθ) = E\n",
      "\n",
      "(cid:2)− log σ(fθ(a+\n",
      "\n",
      "2 ))(cid:3) 1 , a+ (cid:2)− log σ(−fθ(a− 1 , a−\n",
      "\n",
      "(a+ 1 ,a+ + E\n",
      "\n",
      "2 )∼p+(A1,A2)\n",
      "\n",
      "1 ∼p(A),a− a−\n",
      "\n",
      "2 ∼p(A)\n",
      "\n",
      "Contrastive learning is a powerful framework for learning self-supervised represen- tations that generalize well to downstream supervised tasks. We show that multiple existing contrastive learning methods can be reinterpreted as learning kernel func- tions that approximate a ﬁxed positive-pair kernel. We then prove that a simple representation obtained by combining this kernel with PCA provably minimizes the worst-case approximation error of linear predictors, under a straightforward assumption that positive pairs have similar labels. Our analysis is based on a de- composition of the target function in terms of the eigenfunctions of a positive-pair Markov chain, and a surprising equivalence between these eigenfunctions and the output of Kernel PCA\n",
      "=============\n",
      "\n",
      "Question: What is standard contrastive learning?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query: What is standard contrastive learning?\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #0\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "There have been a number of other attempts to unify different contrastive learning techniques in a single theoretical\n",
      "framework. Balestriero & LeCun (2022) describe a uniﬁcation using the framework of spectral embedding methods, and draw\n",
      "connections between SimCLR and Kernel ISOMAP. Tian (2022) provides a game-theoretic uniﬁcation, and shows that\n",
      "contrastive losses are related to PCA in the input space for the special case of deep linear networks.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #1\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Recall that during contrastive learning we have a distribution p(Z) of original examples, and a distribution p(A|Z = z)\n",
      "of augmented views for each original example z. Then, at downstream supervised learning time, we additionally have a\n",
      "distribution p(Y = y|Z = z) of labels y ∈ R for each original example z. (For simplicity we assume that the distribution\n",
      "of p(Z) remains unchanged during the downstream learning step. We also assume y is a scalar; the vector case can be\n",
      "derived by independently estimating each element of Y with a separate target function.)\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #2\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "1  INTRODUCTION  When using a contrastive learning method such as SimCLR (Chen et al., 2020a) for representation\n",
      "learning, the ﬁrst step is to specify the distribution of original examples z ∼ p(Z) within some space Z along with a\n",
      "sampler of augmented views p(A|Z = z) over a potentially different space A. 1 For example, p(Z) might represent a\n",
      "dataset of natural images, and p(A|Z) a random transformation that applies random scaling and color shifts. Contrastive\n",
      "learning then consists of ﬁnding a parameterized mapping (such as a neural network) which maps multiple views of the\n",
      "same image (e.g. draws from a1, a2 ∼ p(A|Z = z) for a ﬁxed z) close together, and unrelated views far apart. This\n",
      "mapping can then be used to deﬁne a representation which is useful for downstream supervised learning.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #3\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "K+(a1, a2) =  p+(a1, a2) p(a1)p(a2)  = (cid:104)φ+(a1), φ+(a2)(cid:105) ,  (1)  where p+(a1, a2) = (cid:80)  φ+(a) =  z\n",
      "p(a1|z)p(a2|z)p(z), and φ+ : A → R|Z| is the transformation (cid:104) p(a|z1)  √  √  √  (cid:105)(cid:62)  p(a|z|Z|)\n",
      "p(z|Z|)  p(z1)  p(a|z2)  p(z2)  · ·  p(a)  p(a)  p(a)  .  (2)  Here, the magnitude of the dot product between vectors\n",
      "φ+(a1) and φ+(a2) reﬂects the relative likelihood of data points a1 and a2 being drawn from a positive pair v.s. a\n",
      "negative pair. In particular, if a1 and a2 have zero probability of being a positive pair, φ+(a1) and φ+(a2) are\n",
      "orthogonal.  As shown in Figure 1, we can thus reinterpret the three contrastive learning methods in Table 1 as kernel\n",
      "learning methods, in that they produce parameterized positive-deﬁnite kernel functions which approximate this positive-\n",
      "pair kernel. By investigating properties of this kernel, we can thus hope to build a better understanding of the\n",
      "behavior of contrastive learning.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #4\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "The success of these representations have led to a variety of theoretical analyses of contrastive learning, including\n",
      "analyses based on conditional independence within latent classes (Saunshi et al., 2019), alignment of hyperspherical\n",
      "embeddings (Wang & Isola, 2020), conditional independence structure with landmark embeddings (Tosh et al., 2021), and\n",
      "spectral analysis of an augmentation graph (HaoChen et al., 2021). Each of these analyses is based on a single choice of\n",
      "contrastive learning objective. In this work, we go further by integrating multiple popular contrastive learning methods\n",
      "into a single framework, and showing that it can be used to build minimax-optimal representations under a\n",
      "straightforward assumption about similarity of labels between positive pairs.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #5\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "2 ))2(cid:105)  1 , a−  Figure 1: The positive-pair ker- nel K+ assigns high similarity to likely positive pairs.\n",
      "Contrastive learning methods learn parame- terized kernels (cid:98)Kθ which assign high similarity to nearby points in a\n",
      "learned embedding space.  Table 1: Existing contrastive learning objectives, reinterpreted as learning parameterized\n",
      "approximations of K+. “Minimum” de- notes the population minimum of the loss over all kernel functions (not necessarily\n",
      "representable using the shown parameterization). C[a1] is a equivalence-class-dependent proportionality constant, with\n",
      "C[a1] = C[a2] whenever p+(a1, a2) > 0. See Appendix B for derivations and discussion of other related objectives.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #6\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "We conclude by studying the behavior of contrastive learning models on two synthetic tasks for which the exact positive-\n",
      "pair kernel is known, and investigating the extent to which the basis of eigenfunctions can be extracted from trained\n",
      "models. As predicted by our theory, we ﬁnd that the same eigenfunctions can be recovered from multiple model\n",
      "parameterizations and losses, although the accuracy depends on both kernel parameterization expressiveness and\n",
      "augmentation strength.  2 CONTRASTIVE LEARNING IS SECRETLY KERNEL LEARNING\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #7\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "We note that applying a rotation matrix to the optimal representation does not affect the expressivity of that\n",
      "representation. It is thus sufﬁcient to identify the subspace spanned by the ﬁrst d principal component projection\n",
      "functions. If the representation dimension d is known in advance, it may be possible to adjust the contrastive learning\n",
      "method to accomplish this without requiring a separate PCA step. In particular, when using the spectral contrastive loss\n",
      "with a d-dimensional linear kernel head, the population minimizer of the loss will exactly correspond to the best\n",
      "d-dimensional approximation of K+. This means that the output layer representation will be exactly the set of principal\n",
      "component projection functions rotated by some orthogonal matrix.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #8\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Indeed, such a minimizer is equivalent to K+ except that it scales the inner product by the value of b(a) for each\n",
      "communicating class. It is still possible to extract the set of Markov chain eigenfunctions from the set of principal\n",
      "components of this kernel, although one must correct for the scaling factor when computing the eigenvalues; see Appendix\n",
      "C.4 for details. Alternatively, one can ensure a unique minimum by combining the NT-Xent/InfoNCE loss with either the\n",
      "spectral or logistic losses (discussed below).  B.2 LOGISTIC LOSSES AND NT-LOGISTIC  Logistic losses have also been\n",
      "proposed for contrastive learning, including the NT-Logistic objective as described in Chen et al. (2020a) and other\n",
      "versions described by Mikolov et al. (2013) and Tosh  16  Published as a conference paper at ICLR 2023  et al. (2021).\n",
      "Such losses take the form  LLogistic(fθ) = E  (cid:2)− log σ(fθ(a+  2 ))(cid:3) 1 , a+ (cid:2)− log σ(−fθ(a− 1 , a−  (a+\n",
      "1 ,a+ + E  2 )∼p+(A1,A2)  1 ∼p(A),a− a−  2 ∼p(A)\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #9\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Contrastive learning is a powerful framework for learning self-supervised represen- tations that generalize well to\n",
      "downstream supervised tasks. We show that multiple existing contrastive learning methods can be reinterpreted as\n",
      "learning kernel func- tions that approximate a ﬁxed positive-pair kernel. We then prove that a simple representation\n",
      "obtained by combining this kernel with PCA provably minimizes the worst-case approximation error of linear predictors,\n",
      "under a straightforward assumption that positive pairs have similar labels. Our analysis is based on a de- composition\n",
      "of the target function in terms of the eigenfunctions of a positive-pair Markov chain, and a surprising equivalence\n",
      "between these eigenfunctions and the output of Kernel PCA\n",
      "................................................................................\n",
      "Response:  Standard contrastive learning aims to learn a mapping that brings multiple views of the same image closer together in a\n",
      "learned embedding space, while pushing unrelated views further apart. This is achieved by minimizing a contrastive loss\n",
      "function that encourages similar representations for augmented views of the same image and discourages similar\n",
      "representations for different images.\n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "ask(\"What is standard contrastive learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "iKEKnEkErLLM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
      "\n",
      "Question: What affects engineering productivity?\n",
      "\n",
      "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
      "\n",
      "Do not try to make up an answer:\n",
      " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
      " - If the context is empty, just say \"I do not know the answer to that.\"\n",
      "\n",
      "=============\n",
      "Conflating throughput with pro- ductivity: One might count the lines of code written by a devel- oper in a certain period and cal- culate a simple output per unit time “productivity” measure like lines of code per minute. Pounds of coal shoveled per hour will tell you which shovelers are the best shovelers; lines of code per minute will not tell you which software developers are the best software developers.\n",
      "\n",
      "A related simplification about en- gineering work is that it is linear and predictable. We find that engineer- ing leaders, in the face of a complex problem, seek to simplify the entire\n",
      "\n",
      "The output of software devel- opment is not known from the start: In part, that’s because the solution is not known ahead of time. It may be well constrained,\n",
      "\n",
      "Assuming the built product is the right product: Some code\n",
      "\n",
      "J A N U A R Y / F E B R U A R Y 2 0 2 3 | I E E E S O F T W A R E\n",
      "\n",
      "25\n",
      "\n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "\n",
      "So to recap: developers are hu- man, and thus the things that make being a human harder or easier also make being an engineer harder or easier. This isn’t to say that there are no factors that specifically affect software developers more than they affect other humans. Flaky tests, build speeds, and technical debt are domain-specific phenomena that have lots to do with software devel- opment as an activity. But the influ- ence of domain-specific factors is always accompanied by (and often small in comparison to) the influence of those factors that are general to humans.\n",
      "\n",
      "Similar to the assertion that devel- opers are human, the assertion that software engineering work is nonlin- ear and unpredictable is often met with agreement. But again, we see at- tempts to simplify engineering work to make the problem of engineering productivity more tractable. This sim- plification leads to failures to treat engineering as appropriately complex and creative in practice. For example:\n",
      "\n",
      "The characteristics and limita- tions of human decision-making processes and memory: Humans reason in different ways at dif- ferent times and may select a reasoning approach based on context (for example, time pres- sure and incentives can push a person into thinking “fast” or “slow”1), and the different modes are subject to different strengths and weaknesses.2 People also have limits to their working memory,3 so anything we can do to bring the right information to their attention at the right time can improve productivity.\n",
      "\n",
      "The complexity of a task and whether that task is essentially\n",
      "\n",
      "I E E E S O F T W A R E | W W W. C O M P U T E R . O R G / S O F T W A R E | @ I E E E S O F T W A R E\n",
      "\n",
      "24\n",
      "\n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "\n",
      "At the beginning of this article, we mentioned that we study what makes engineers productive and happy. Pro- ductive engineers might be unhappy and—despite feeling productive— decide to go someplace else for a job. They’re human after all. Despite sys- tematic selection processes, develop- ers are not interchangeable. When a senior, long-tenured engineer leaves an organization, it is impossible to simply drop in a replacement en- gineer who has been in cold stor- age. Attrition (whether on good or bad terms) has a cost in productivity and resources, and when productiv- ity measurements (especially myopic and inappropriate measurements, like mere throughput) are foisted upon developers, they are likely to become unhappy.\n",
      "\n",
      "4. Provide specific, prescriptive task instructions to each worker and monitor them to ensure they execute as directed.\n",
      "\n",
      "The organizational and busi- ness context in which the hu- man completes the task: The way that projects and people are organized can impact productiv- ity (Conway’s law4,5). Orga- nizational incentives can also impact productivity: if delivering software fast is rewarded more than delivering software that is reliable, that pressure will shape how work gets done (and what the output looks like). It also shapes the very definition of pro- ductivity in that context. • The environmental, social, and cultural context in which the human completes the task: Whether they do it overtly or co- vertly, individuals bring a whole self to work. They bring their sex, gender, race, ethnicity, na- tionality, religious background, height, weight, personal beliefs, age, and choices in hairstyle, clothing, and music to work. This introduces a whole other level of complexity to their work and work context\n",
      "\n",
      "developer productivity?\n",
      "\n",
      "1. Engineers are humans, and hu- mans are inherently messy. 2. Engineering is a complex and creative task.\n",
      "\n",
      "3. Measuring the productivity of any knowledge worker is gener- ally a hard problem.\n",
      "\n",
      "Developer productivity for hu- mans is what our new column is about: how we understand it, how we measure it, and how we improve it. In this article, we’ll talk about why this problem is so difficult, and in future installments, we’ll get into specific aspects of developer produc- tivity and consider how we might— collectively—improve developer productivity across the industry.\n",
      "\n",
      "0 7 4 0 - 7 4 5 9 / 2 2 © 2 0 2 2 I E E E\n",
      "\n",
      "J A N U A R Y / F E B R U A R Y 2 0 2 3 | I E E E S O F T W A R E\n",
      "\n",
      "23\n",
      "\n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "\n",
      "complex or accidentally so4: We can reduce complexity (and therefore increase productiv- ity) by removing accidental complexity.\n",
      "\n",
      "We need to think about mea- suring productivity in a holistic and multifaceted way, not in a re- ductionist, unidimensional way.10 Accordingly, we need to measure productivity using more than one metric, and we need frameworks for selecting metrics (for example, SPACE from Forsgren et al.)11 that enable us to understand tradeoffs. We must think about productiv- ity both in the short term and the long term; for example, we need to understand the effects that biases against underrepresented groups have on developer productivity in terms of getting code submitted and also on retaining skilled employees by treating them fairly and affording them the same opportunities that others enjoy.\n",
      "\n",
      "I n future installments of this\n",
      "\n",
      "However, engineering is not a lin- ear or predictable process, and when it is, we consider it “toil” or “boiler- plate” and automate away the predict- ability. This is diametrically opposite of stamping out parts in a machine shop, which is linear and predictable (within some tolerances). So is paint- ing a house, or assembling a rocket, or shoveling coal. Additionally, suc- cess in these activities is measur- able and unambiguously understood. More coal shoveled is better than less coal shoveled (strictly from a coal-shoveling-productivity point of view). Further, the solution in these examples is about the production of uniform, interchangeable outputs. One pound of coal is pretty much in- distinguishable from any other. Not so for code.\n",
      "\n",
      "There are many things that, in com- bination, will influence how produc- tive a human will be at a task.\n",
      "\n",
      "It’s widely accepted that having a faster build speed improves developer productivity. We see evidence that this effect does not happen because a developer sits idly by, waiting for the build to complete. Rather, it occurs because when a build is suffi- ciently fast, the developer is likely to stay in flow and retain the context of the task. If a build is too slow, a developer will make a very human decision and context switch away to a new task. The developer will also take longer to resume the task when he or she switches back because of the need to regain the context of the task. • Developers, as humans, are sub- ject to unconscious biases that may affect how they interact\n",
      "\n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "\n",
      "Editor: Ciera Jaspan Google ciera@google.com\n",
      "\n",
      "Editor: Collin Green Google colling@google.com\n",
      "\n",
      "A Human-Centered Approach to Developer Productivity\n",
      "\n",
      "Ciera Jaspan and Collin Green\n",
      "\n",
      "From the Editors\n",
      "\n",
      "The “Developer Productivity for Humans” column aims to draw attention to\n",
      "\n",
      "advances and challenges in research and practice in tools and practices that\n",
      "\n",
      "help improve developers’ day-to-day tasks. In this column, we reinforce that\n",
      "\n",
      "software engineers and developers are human and productivity tools should\n",
      "\n",
      "support making their jobs easier as opposed to turning them into productivity\n",
      "\n",
      "machines. We share our experiences and expertise and welcome your contribu-\n",
      "\n",
      "tions and feedback.\n",
      "\n",
      "WE LEAD A mixed-methods re- search team at Google that seeks to understand what makes engi- neers productive and happy. We explore the impact of different engineering tools, infrastructure, processes, and best practices on engineering productivity.\n",
      "=============\n",
      "\n",
      "Question: What affects engineering productivity?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query: What affects engineering productivity?\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #0\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Conflating throughput with pro- ductivity: One might count the lines of code written by a devel- oper in a certain\n",
      "period and cal- culate a simple output per unit time “productivity” measure like lines of code per minute. Pounds of\n",
      "coal shoveled per hour will tell you which shovelers are the best shovelers; lines of code per minute will not tell you\n",
      "which software developers are the best software developers.  A related simplification about en- gineering work is that\n",
      "it is linear and predictable. We find that engineer- ing leaders, in the face of a complex problem, seek to simplify the\n",
      "entire  The output of software devel- opment is not known from the start: In part, that’s because the solution is not\n",
      "known ahead of time. It may be well constrained,  Assuming the built product is the right product: Some code  J A N U A\n",
      "R Y / F E B R U A R Y 2 0 2 3 | I E E E S O F T W A R E  25  DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #1\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "So to recap: developers are hu- man, and thus the things that make being a human harder or easier also make being an\n",
      "engineer harder or easier. This isn’t to say that there are no factors that specifically affect software developers more\n",
      "than they affect other humans. Flaky tests, build speeds, and technical debt are domain-specific phenomena that have\n",
      "lots to do with software devel- opment as an activity. But the influ- ence of domain-specific factors is always\n",
      "accompanied by (and often small in comparison to) the influence of those factors that are general to humans.  Similar to\n",
      "the assertion that devel- opers are human, the assertion that software engineering work is nonlin- ear and unpredictable\n",
      "is often met with agreement. But again, we see at- tempts to simplify engineering work to make the problem of\n",
      "engineering productivity more tractable. This sim- plification leads to failures to treat engineering as appropriately\n",
      "complex and creative in practice. For example:\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #2\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "The characteristics and limita- tions of human decision-making processes and memory: Humans reason in different ways at\n",
      "dif- ferent times and may select a reasoning approach based on context (for example, time pres- sure and incentives can\n",
      "push a person into thinking “fast” or “slow”1), and the different modes are subject to different strengths and\n",
      "weaknesses.2 People also have limits to their working memory,3 so anything we can do to bring the right information to\n",
      "their attention at the right time can improve productivity.  The complexity of a task and whether that task is\n",
      "essentially  I E E E S O F T W A R E | W W W. C O M P U T E R . O R G / S O F T W A R E | @ I E E E S O F T W A R E  24\n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #3\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "At the beginning of this article, we mentioned that we study what makes engineers productive and happy. Pro- ductive\n",
      "engineers might be unhappy and—despite feeling productive— decide to go someplace else for a job. They’re human after\n",
      "all. Despite sys- tematic selection processes, develop- ers are not interchangeable. When a senior, long-tenured\n",
      "engineer leaves an organization, it is impossible to simply drop in a replacement en- gineer who has been in cold stor-\n",
      "age. Attrition (whether on good or bad terms) has a cost in productivity and resources, and when productiv- ity\n",
      "measurements (especially myopic and inappropriate measurements, like mere throughput) are foisted upon developers, they\n",
      "are likely to become unhappy.  4. Provide specific, prescriptive task instructions to each worker and monitor them to\n",
      "ensure they execute as directed.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #4\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "The organizational and busi- ness context in which the hu- man completes the task: The way that projects and people are\n",
      "organized can impact productiv- ity (Conway’s law4,5). Orga- nizational incentives can also impact productivity: if\n",
      "delivering software fast is rewarded more than delivering software that is reliable, that pressure will shape how work\n",
      "gets done (and what the output looks like). It also shapes the very definition of pro- ductivity in that context. • The\n",
      "environmental, social, and cultural context in which the human completes the task: Whether they do it overtly or co-\n",
      "vertly, individuals bring a whole self to work. They bring their sex, gender, race, ethnicity, na- tionality, religious\n",
      "background, height, weight, personal beliefs, age, and choices in hairstyle, clothing, and music to work. This\n",
      "introduces a whole other level of complexity to their work and work context\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #5\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "developer productivity?  1. Engineers are humans, and hu- mans are inherently messy. 2. Engineering is a complex and\n",
      "creative task.  3. Measuring the productivity of any knowledge worker is gener- ally a hard problem.  Developer\n",
      "productivity for hu- mans is what our new column is about: how we understand it, how we measure it, and how we improve\n",
      "it. In this article, we’ll talk about why this problem is so difficult, and in future installments, we’ll get into\n",
      "specific aspects of developer produc- tivity and consider how we might— collectively—improve developer productivity\n",
      "across the industry.  0 7 4 0 - 7 4 5 9 / 2 2 © 2 0 2 2 I E E E  J A N U A R Y / F E B R U A R Y 2 0 2 3 | I E E E S O F\n",
      "T W A R E  23  DEVELOPER PRODUCTIVITY FOR HUMANS  complex or accidentally so4: We can reduce complexity (and therefore\n",
      "increase productiv- ity) by removing accidental complexity.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #6\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "We need to think about mea- suring productivity in a holistic and multifaceted way, not in a re- ductionist,\n",
      "unidimensional way.10 Accordingly, we need to measure productivity using more than one metric, and we need frameworks\n",
      "for selecting metrics (for example, SPACE from Forsgren et al.)11 that enable us to understand tradeoffs. We must think\n",
      "about productiv- ity both in the short term and the long term; for example, we need to understand the effects that\n",
      "biases against underrepresented groups have on developer productivity in terms of getting code submitted and also on\n",
      "retaining skilled employees by treating them fairly and affording them the same opportunities that others enjoy.  I n\n",
      "future installments of this\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #7\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "However, engineering is not a lin- ear or predictable process, and when it is, we consider it “toil” or “boiler- plate”\n",
      "and automate away the predict- ability. This is diametrically opposite of stamping out parts in a machine shop, which is\n",
      "linear and predictable (within some tolerances). So is paint- ing a house, or assembling a rocket, or shoveling coal.\n",
      "Additionally, suc- cess in these activities is measur- able and unambiguously understood. More coal shoveled is better\n",
      "than less coal shoveled (strictly from a coal-shoveling-productivity point of view). Further, the solution in these\n",
      "examples is about the production of uniform, interchangeable outputs. One pound of coal is pretty much in-\n",
      "distinguishable from any other. Not so for code.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #8\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "There are many things that, in com- bination, will influence how produc- tive a human will be at a task.  It’s widely\n",
      "accepted that having a faster build speed improves developer productivity. We see evidence that this effect does not\n",
      "happen because a developer sits idly by, waiting for the build to complete. Rather, it occurs because when a build is\n",
      "suffi- ciently fast, the developer is likely to stay in flow and retain the context of the task. If a build is too slow,\n",
      "a developer will make a very human decision and context switch away to a new task. The developer will also take longer\n",
      "to resume the task when he or she switches back because of the need to regain the context of the task. • Developers, as\n",
      "humans, are sub- ject to unconscious biases that may affect how they interact\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #9\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS  Editor: Ciera Jaspan Google ciera@google.com  Editor: Collin Green Google\n",
      "colling@google.com  A Human-Centered Approach to Developer Productivity  Ciera Jaspan and Collin Green  From the Editors\n",
      "The “Developer Productivity for Humans” column aims to draw attention to  advances and challenges in research and\n",
      "practice in tools and practices that  help improve developers’ day-to-day tasks. In this column, we reinforce that\n",
      "software engineers and developers are human and productivity tools should  support making their jobs easier as opposed\n",
      "to turning them into productivity  machines. We share our experiences and expertise and welcome your contribu-  tions\n",
      "and feedback.  WE LEAD A mixed-methods re- search team at Google that seeks to understand what makes engi- neers\n",
      "productive and happy. We explore the impact of different engineering tools, infrastructure, processes, and best\n",
      "practices on engineering productivity.\n",
      "................................................................................\n",
      "Response:  There are many factors that affect engineering productivity, including:  - The characteristics and limitations of human\n",
      "decision-making processes and memory. - The complexity of a task and whether that task is essentially complex or\n",
      "accidentally so. - The organizational and business context in which the human completes the task. - The environmental,\n",
      "social, and cultural context in which the human completes the task. - The availability of specific, prescriptive task\n",
      "instructions and monitoring to ensure they are executed as directed.\n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "ask(\"What affects engineering productivity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DbFOWgJMr9Dl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
      "\n",
      "Question: Why are humans preferred as software developers?\n",
      "\n",
      "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
      "\n",
      "Do not try to make up an answer:\n",
      " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
      " - If the context is empty, just say \"I do not know the answer to that.\"\n",
      "\n",
      "=============\n",
      "There are two good reasons to think of developers as humans. First, developers strongly prefer that their humanity is acknowledged and re- spected (a key indicator of their hu- manity, in fact, is their preference for others to recognize it). Second, if we want to understand what makes devel- opers more or less productive, we need to understand what makes humans more or less productive, with software development as a special case.\n",
      "\n",
      "Software Developers Are Humans Software developers are humans. All of them. It seems that this should be an uncontroversial assertion. Indeed, when we assert it, no one ever dis- agrees. And yet, we find ourselves making this assertion on a regular ba- sis. Why? Because despite wide agree- ment with the assertion itself, many people behave as though developers were not humans, but rather nonhu- man components in a larger system— cogs in a machine, if you will.\n",
      "\n",
      "or election results) can change how an individual interacts with others and whether that person might be distracted by outside forces or more sharply focused on the tasks at hand.\n",
      "\n",
      "The team of other humans that one needs to work with to ac- complish the task: In addition to the dynamics of the team mem- bers, factors such as geographic and time zone dispersion will affect how people communicate, and factors such as a mix of prior experience affect techni- cal mentorship and institutional knowledge.\n",
      "\n",
      "Software Engineering Is a Complex, Creative Endeavor One good reason that we continue to employ humans as software develop- ers, despite all of the messiness de- scribed previously, is that humans are good creative problem solvers. Engi- neering is an inherently creative en- deavor in that it involves finding novel solutions to complex problems. Not every solution is novel, and not every problem is complex, of course, but applying known solutions to simple problems doesn’t really require “engi- neering” in any meaningful sense.\n",
      "\n",
      "Software engineering is not algorithmic: There is no pre- scribed set of steps that will take one from having no function- ing code to having functioning code. While there are general processes or best practices (agile methods, test-driven develop- ment, modular designs, fault- tolerant architectures), engineers adapt to the current problem and take alternative paths as necessary.\n",
      "\n",
      "Conflating throughput with pro- ductivity: One might count the lines of code written by a devel- oper in a certain period and cal- culate a simple output per unit time “productivity” measure like lines of code per minute. Pounds of coal shoveled per hour will tell you which shovelers are the best shovelers; lines of code per minute will not tell you which software developers are the best software developers.\n",
      "\n",
      "A related simplification about en- gineering work is that it is linear and predictable. We find that engineer- ing leaders, in the face of a complex problem, seek to simplify the entire\n",
      "\n",
      "The output of software devel- opment is not known from the start: In part, that’s because the solution is not known ahead of time. It may be well constrained,\n",
      "\n",
      "Assuming the built product is the right product: Some code\n",
      "\n",
      "J A N U A R Y / F E B R U A R Y 2 0 2 3 | I E E E S O F T W A R E\n",
      "\n",
      "25\n",
      "\n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "\n",
      "24\n",
      "\n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "\n",
      "with others through bug reports, design docs, and code reviews. These biases may improve or degrade their own—and oth- ers’—productivity and experi- ence. While one could argue that they constitute a “human prob- lem,” we can also mitigate such biases with tool changes, such as anonymous code reviews.6\n",
      "\n",
      "software engineering process: ideas are generated, designed, implemented, tested, experimented, launched, and maintained. Even engineers are in- clined to describe these activities as algorithmic in the most fundamental sense: a prescribed series of steps will inevitably move things from a starting state to a solution.\n",
      "\n",
      "and it may look a lot like solu- tions that have been built in the past, but even where code reuse is extensive and a developer draws on bits and pieces from others’ solutions, the output of a software development task is unique.\n",
      "\n",
      "So to recap: developers are hu- man, and thus the things that make being a human harder or easier also make being an engineer harder or easier. This isn’t to say that there are no factors that specifically affect software developers more than they affect other humans. Flaky tests, build speeds, and technical debt are domain-specific phenomena that have lots to do with software devel- opment as an activity. But the influ- ence of domain-specific factors is always accompanied by (and often small in comparison to) the influence of those factors that are general to humans.\n",
      "\n",
      "Similar to the assertion that devel- opers are human, the assertion that software engineering work is nonlin- ear and unpredictable is often met with agreement. But again, we see at- tempts to simplify engineering work to make the problem of engineering productivity more tractable. This sim- plification leads to failures to treat engineering as appropriately complex and creative in practice. For example:\n",
      "\n",
      "25\n",
      "\n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "\n",
      "that gets submitted and deployed is terrible. It might have per- formance issues or introduce bugs. It might be brittle or scale poorly. It might be difficult to comprehend, modify, or main- tain. It might even work per- fectly, yet be the wrong market fit. Part of productivity is not just the output but whether it was the right output to build.\n",
      "\n",
      "systematically for at least a century, haven’t we progressed further with measuring developer productivity? Yes. And no.\n",
      "\n",
      "entire books dedicated to the decom- position and modularization of code such that tasks can be more easily delegated across a team with lower communication overhead. This de- composition, though, is itself an en- gineering task and is arguably much more difficult and time consuming than the completion of the decom- posed work.\n",
      "\n",
      "These problems are not specific to software development, yet they af- fect a developer’s ability to be pro- ductive. Too often, however, people seem desperate to separate “human problems” from “technical prob- lems.” There is a persistent belief that “human problems” are tricky and relegated to human resources and psychology departments, while “technical problems” are somehow more tractable. Yet we don’t see any difference between the two: “tech- nical problems” frequently require understanding human decision-mak- ing processes and performance, and “human problems” can sometimes be addressed by technical solutions. Consider some examples of how they intertwine.\n",
      "\n",
      "The characteristics and limita- tions of human decision-making processes and memory: Humans reason in different ways at dif- ferent times and may select a reasoning approach based on context (for example, time pres- sure and incentives can push a person into thinking “fast” or “slow”1), and the different modes are subject to different strengths and weaknesses.2 People also have limits to their working memory,3 so anything we can do to bring the right information to their attention at the right time can improve productivity.\n",
      "\n",
      "The complexity of a task and whether that task is essentially\n",
      "\n",
      "I E E E S O F T W A R E | W W W. C O M P U T E R . O R G / S O F T W A R E | @ I E E E S O F T W A R E\n",
      "\n",
      "24\n",
      "\n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "\n",
      "There are many things that, in com- bination, will influence how produc- tive a human will be at a task.\n",
      "\n",
      "It’s widely accepted that having a faster build speed improves developer productivity. We see evidence that this effect does not happen because a developer sits idly by, waiting for the build to complete. Rather, it occurs because when a build is suffi- ciently fast, the developer is likely to stay in flow and retain the context of the task. If a build is too slow, a developer will make a very human decision and context switch away to a new task. The developer will also take longer to resume the task when he or she switches back because of the need to regain the context of the task. • Developers, as humans, are sub- ject to unconscious biases that may affect how they interact\n",
      "=============\n",
      "\n",
      "Question: Why are humans preferred as software developers?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query: Why are humans preferred as software developers?\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #0\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "There are two good reasons to think of developers as humans. First, developers strongly prefer that their humanity is\n",
      "acknowledged and re- spected (a key indicator of their hu- manity, in fact, is their preference for others to recognize\n",
      "it). Second, if we want to understand what makes devel- opers more or less productive, we need to understand what makes\n",
      "humans more or less productive, with software development as a special case.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #1\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Software Developers Are Humans Software developers are humans. All of them. It seems that this should be an\n",
      "uncontroversial assertion. Indeed, when we assert it, no one ever dis- agrees. And yet, we find ourselves making this\n",
      "assertion on a regular ba- sis. Why? Because despite wide agree- ment with the assertion itself, many people behave as\n",
      "though developers were not humans, but rather nonhu- man components in a larger system— cogs in a machine, if you will.\n",
      "or election results) can change how an individual interacts with others and whether that person might be distracted by\n",
      "outside forces or more sharply focused on the tasks at hand.  The team of other humans that one needs to work with to\n",
      "ac- complish the task: In addition to the dynamics of the team mem- bers, factors such as geographic and time zone\n",
      "dispersion will affect how people communicate, and factors such as a mix of prior experience affect techni- cal\n",
      "mentorship and institutional knowledge.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #2\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Software Engineering Is a Complex, Creative Endeavor One good reason that we continue to employ humans as software\n",
      "develop- ers, despite all of the messiness de- scribed previously, is that humans are good creative problem solvers.\n",
      "Engi- neering is an inherently creative en- deavor in that it involves finding novel solutions to complex problems. Not\n",
      "every solution is novel, and not every problem is complex, of course, but applying known solutions to simple problems\n",
      "doesn’t really require “engi- neering” in any meaningful sense.  Software engineering is not algorithmic: There is no\n",
      "pre- scribed set of steps that will take one from having no function- ing code to having functioning code. While there\n",
      "are general processes or best practices (agile methods, test-driven develop- ment, modular designs, fault- tolerant\n",
      "architectures), engineers adapt to the current problem and take alternative paths as necessary.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #3\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Conflating throughput with pro- ductivity: One might count the lines of code written by a devel- oper in a certain\n",
      "period and cal- culate a simple output per unit time “productivity” measure like lines of code per minute. Pounds of\n",
      "coal shoveled per hour will tell you which shovelers are the best shovelers; lines of code per minute will not tell you\n",
      "which software developers are the best software developers.  A related simplification about en- gineering work is that\n",
      "it is linear and predictable. We find that engineer- ing leaders, in the face of a complex problem, seek to simplify the\n",
      "entire  The output of software devel- opment is not known from the start: In part, that’s because the solution is not\n",
      "known ahead of time. It may be well constrained,  Assuming the built product is the right product: Some code  J A N U A\n",
      "R Y / F E B R U A R Y 2 0 2 3 | I E E E S O F T W A R E  25  DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #4\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "24  DEVELOPER PRODUCTIVITY FOR HUMANS  with others through bug reports, design docs, and code reviews. These biases may\n",
      "improve or degrade their own—and oth- ers’—productivity and experi- ence. While one could argue that they constitute a\n",
      "“human prob- lem,” we can also mitigate such biases with tool changes, such as anonymous code reviews.6  software\n",
      "engineering process: ideas are generated, designed, implemented, tested, experimented, launched, and maintained. Even\n",
      "engineers are in- clined to describe these activities as algorithmic in the most fundamental sense: a prescribed series\n",
      "of steps will inevitably move things from a starting state to a solution.  and it may look a lot like solu- tions that\n",
      "have been built in the past, but even where code reuse is extensive and a developer draws on bits and pieces from\n",
      "others’ solutions, the output of a software development task is unique.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #5\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "So to recap: developers are hu- man, and thus the things that make being a human harder or easier also make being an\n",
      "engineer harder or easier. This isn’t to say that there are no factors that specifically affect software developers more\n",
      "than they affect other humans. Flaky tests, build speeds, and technical debt are domain-specific phenomena that have\n",
      "lots to do with software devel- opment as an activity. But the influ- ence of domain-specific factors is always\n",
      "accompanied by (and often small in comparison to) the influence of those factors that are general to humans.  Similar to\n",
      "the assertion that devel- opers are human, the assertion that software engineering work is nonlin- ear and unpredictable\n",
      "is often met with agreement. But again, we see at- tempts to simplify engineering work to make the problem of\n",
      "engineering productivity more tractable. This sim- plification leads to failures to treat engineering as appropriately\n",
      "complex and creative in practice. For example:\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #6\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "25  DEVELOPER PRODUCTIVITY FOR HUMANS  that gets submitted and deployed is terrible. It might have per- formance issues\n",
      "or introduce bugs. It might be brittle or scale poorly. It might be difficult to comprehend, modify, or main- tain. It\n",
      "might even work per- fectly, yet be the wrong market fit. Part of productivity is not just the output but whether it was\n",
      "the right output to build.  systematically for at least a century, haven’t we progressed further with measuring\n",
      "developer productivity? Yes. And no.  entire books dedicated to the decom- position and modularization of code such that\n",
      "tasks can be more easily delegated across a team with lower communication overhead. This de- composition, though, is\n",
      "itself an en- gineering task and is arguably much more difficult and time consuming than the completion of the decom-\n",
      "posed work.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #7\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "These problems are not specific to software development, yet they af- fect a developer’s ability to be pro- ductive. Too\n",
      "often, however, people seem desperate to separate “human problems” from “technical prob- lems.” There is a persistent\n",
      "belief that “human problems” are tricky and relegated to human resources and psychology departments, while “technical\n",
      "problems” are somehow more tractable. Yet we don’t see any difference between the two: “tech- nical problems” frequently\n",
      "require understanding human decision-mak- ing processes and performance, and “human problems” can sometimes be addressed\n",
      "by technical solutions. Consider some examples of how they intertwine.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #8\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "The characteristics and limita- tions of human decision-making processes and memory: Humans reason in different ways at\n",
      "dif- ferent times and may select a reasoning approach based on context (for example, time pres- sure and incentives can\n",
      "push a person into thinking “fast” or “slow”1), and the different modes are subject to different strengths and\n",
      "weaknesses.2 People also have limits to their working memory,3 so anything we can do to bring the right information to\n",
      "their attention at the right time can improve productivity.  The complexity of a task and whether that task is\n",
      "essentially  I E E E S O F T W A R E | W W W. C O M P U T E R . O R G / S O F T W A R E | @ I E E E S O F T W A R E  24\n",
      "DEVELOPER PRODUCTIVITY FOR HUMANS\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #9\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-human-centered-approach-to-developer-productivity.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "There are many things that, in com- bination, will influence how produc- tive a human will be at a task.  It’s widely\n",
      "accepted that having a faster build speed improves developer productivity. We see evidence that this effect does not\n",
      "happen because a developer sits idly by, waiting for the build to complete. Rather, it occurs because when a build is\n",
      "suffi- ciently fast, the developer is likely to stay in flow and retain the context of the task. If a build is too slow,\n",
      "a developer will make a very human decision and context switch away to a new task. The developer will also take longer\n",
      "to resume the task when he or she switches back because of the need to regain the context of the task. • Developers, as\n",
      "humans, are sub- ject to unconscious biases that may affect how they interact\n",
      "................................................................................\n",
      "Response:  Humans are preferred as software developers because they are good creative problem solvers. Software engineering is a\n",
      "complex, creative endeavor that involves finding novel solutions to complex problems. While there are general processes\n",
      "or best practices, engineers adapt to the current problem and take alternative paths as necessary.\n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "ask(\"Why are humans preferred as software developers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gWFWKQ98sKLv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
      "\n",
      "Question: How does semantic parsing work with low resource languages?\n",
      "\n",
      "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
      "\n",
      "Do not try to make up an answer:\n",
      " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
      " - If the context is empty, just say \"I do not know the answer to that.\"\n",
      "\n",
      "=============\n",
      "2 Background\n",
      "\n",
      "In this section, we provide an overview of semantic parsing and prior translation-based methods for creating multilingual semantic parsing datasets.\n",
      "\n",
      "2.1 Semantic Parsing\n",
      "\n",
      "Semantic parsing is the task of mapping text queries to their meaning representations or logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013). We focus on task-oriented semantic parsing (Gupta et al., 2018) where the user utterance needs to be parsed into a high-level intent specifying the overall goal, and ﬁne-grained slots containing details about the utterance. The intents and slots come from a task-speciﬁc vocabulary. For example, given an utterance x: “How is the rainfall today?”, the parser should generate the logical-form y: [IN:GET_WEATHER [SL:ATTRIBUTE rainfall] [SL:DATE today ] ]\n",
      "\n",
      "Here, IN:GET_WEATHER is the high-level intent, SL:ATTRIBUTE and SL:DATE are the slots that spec- ify details about the intent. We refer to the logical-\n",
      "\n",
      "9 Ethical Considerations\n",
      "\n",
      "We utilize large language models to translate datasets initially available in English into several languages. The real-world deployment of models trained on LLM-translated data should undergo a careful review of any harmful biases. However, the LLM-translated data and the logical-forms gen- erated by a semantic parser are not user-facing, thus a smaller risk of any direct harms. The in- tended users of any semantic parsing model must be made aware that the answers returned by the model could be incorrect, more so for user-queries in low-resource languages. We do not immediately foresee any serious negative implications of the speciﬁc contributions that we make in this work.\n",
      "\n",
      "10 Acknowledgment\n",
      "\n",
      "We thank Rahul Goel, Slav Petrov, and Kartikeya Badola for discussions and their feedback on an ear-\n",
      "\n",
      "lier draft of this paper. We thank Massimo Nicosia for sharing the TAF translated datasets and helpful discussions during this project.\n",
      "\n",
      "References\n",
      "\n",
      "and\n",
      "\n",
      "Few-Shot\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "We present a method of utilizing large language models (LLMs) for bootstrapping multilingual se- mantic parsers across several languages. In compar- ison to using off-the-shelf translation services that rely on signiﬁcant amounts of human supervision, we demonstrate that prompting self-supervised LLMs can be a more effective and scalable alter-\n",
      "\n",
      "native for dataset translation. We ﬁnd that gen- erating multiple diverse translations using sam- pling techniques followed by consistency-based ﬁltering make the translated datasets more effec- tive for training multilingual semantic parsers. On 41 out of 50 typologically diverse languages within two large datasets spanning several domains, our method outperforms a strong translate-train method that utilizes a supervised translation service.\n",
      "\n",
      "8 Limitations\n",
      "\n",
      "., 2021; Gritta et al., 2022; Wang et al., 2022). The projection step is often learned independent of the translation service, resulting in poor generalization across languages. In this work we aim to utilize the few-shot gen- eralization abilities exhibited by large language models (LLMs) (Brown et al., 2020; Chowdh- ery et al., 2022; Scao et al., 2022) for bootstrap- ping semantic parsing datasets across ﬁfty lan- guages. We propose a recipe of using LLMs to translate an English semantic parsing dataset containing (utterance, logical-form) pairs: Deng = {(xi eng, yi eng)} into a corresponding dataset in a target language: Dtgt = {(xi tgt, yi tgt)}. The gen- erated dataset Dtgt is then used to train a semantic parser in the target language. Our method uses a small amount of manually translated semantic parsing examples to teach the LLM how to trans- late English examples in the target language via in-context learning (Min et al., 2022).\n",
      "\n",
      "We consider the problem of automatically syn- thesizing semantic parsing datasets across several ∗Work done during an internship at Google Research\n",
      "\n",
      "Figure 1 describes our data-translation pipeline which we refer to as LLM-T (§ 3). In contrast\n",
      "\n",
      "Figure 1: Proposed semantic parsing data translation pipeline using LLMs (§ 3): With the help of human translators, we ﬁrst collect translations of a small seed set of English examples in the Target Language (e.g. Hindi; § 3.1). Given a new English example, a small subset from this initial seed set of examples with their respective translations is chosen to prompt the LLM (§ 3.2). The prompted LLM translates the given English example in the Target Language. We repeat this process for each example in the English training data to generate a training dataset in the Target Language. To ensure high-quality of the resulting dataset, we generate diverse translations via top-p (nucleus) sampling (§ 3.3) and apply consistency ﬁltering (§ 3.4).\n",
      "\n",
      ". The translated examples are then used to train a downstream mul- tilingual semantic parser. For example, Nicosia et al. (2021) implement the project step by training a ﬁller module on English data to ﬁll slot-values in a logical-form signature by copying spans from the utterance. During inference, the trained ﬁller module is then used in a zero-shot manner to ﬁll logical-form signatures with spans copied from the translated utterances.\n",
      "\n",
      "[SL:ATTRIBUTE\n",
      "\n",
      "rainfall]\n",
      "\n",
      "Semantic Parsing Model We use a pre-trained mT5-Large checkpoint (1.2B parameters) to initial- ize the downstream semantic parsing models that map utterances in the input to logical-forms in the output. We ﬁnetune the mT5 model on the original English dataset mixed with the translated datasets in target languages. We train using the Adafactor optimizer (Shazeer and Stern, 2018) with a ﬁxed learning rate of 1e−3 and a batch size of 256, for 30K steps using the T5X library (Roberts et al., 2022) on 64 TPU-v3 chips. Examples from each language are sampled uniformly for batch creation. For model selection, we choose the best perform- ing checkpoint as per the dev splits and report our results on the test splits.\n",
      "\n",
      "5 Results and Analysis\n",
      "\n",
      "We ﬁrst present downstream performance of seman- tic parsing models trained on data generated by our method (§ 5.1) and compare with zero-shot setting, few-shot setting, and the TAF method (Nicosia et al., 2021). We then compare our method against the “full-shot” skyline where we utilize the origi- nal training datasets that were manually translated with the help of human annotators in the target languages (§ 5.2). We then present an analysis of different design choices that result in effective data translation using LLM-T (§ 5.3). Finally, we present an error analysis to show the key sources of disagreements between the parser predictions and the ground truth (§ 5.4). All the experiments use our corrected EM metric (§ 4; Evaluation Metric).\n",
      "\n",
      "5.1 Evaluation on MTOP and MASSIVE\n",
      "\n",
      "Despite cross-lingual generalization demon- strated by pre-trained multilingual models, the translate-train paradigm of transferring En- glish datasets across multiple languages re- mains to be a key mechanism for training task- speciﬁc multilingual models. However, for many low-resource languages, the availability of a reliable translation service entails signiﬁ- cant amounts of costly human-annotated trans- lation pairs. Further, translation services may continue to be brittle due to domain mismatch between task-speciﬁc input text and general- purpose text used for training translation mod- els. For multilingual semantic parsing, we demonstrate the effectiveness and ﬂexibility offered by large language models (LLMs) for translating English datasets into several lan- guages via few-shot prompting\n",
      "\n",
      "languages. Semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013) is the task of mapping natural language text into an executable logical-form. For exam- ple, given a user instruction (x) : “Wake me up by 5 am”, mapping it to the logical-form (y): [IN:CREATE_ALARM [SL:DATE_TIME 5 am ]]. Manual annotation of queries with their logical forms requires human expertise which makes data collection across multiple languages challenging. A common approach to automatic multilingual dataset creation is translating existing English datasets into target languages. Prior methods uti- lize an off-the-shelf machine translation model for translating the English utterance into the target lan- guage xeng → xtgt, followed by projecting lan- guage speciﬁc components in the English logical- form yeng to obtain the logical-form ytgt in the tar- get language (Moradshahi et al., 2020, 2021; Xia and Monti, 2021; Nicosia et al., 2021; Gritta et al., 2022; Wang et al\n",
      "=============\n",
      "\n",
      "Question: How does semantic parsing work with low resource languages?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query: How does semantic parsing work with low resource languages?\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #0\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "2 Background  In this section, we provide an overview of semantic parsing and prior translation-based methods for\n",
      "creating multilingual semantic parsing datasets.  2.1 Semantic Parsing  Semantic parsing is the task of mapping text\n",
      "queries to their meaning representations or logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant\n",
      "et al., 2013). We focus on task-oriented semantic parsing (Gupta et al., 2018) where the user utterance needs to be\n",
      "parsed into a high-level intent specifying the overall goal, and ﬁne-grained slots containing details about the\n",
      "utterance. The intents and slots come from a task-speciﬁc vocabulary. For example, given an utterance x: “How is the\n",
      "rainfall today?”, the parser should generate the logical-form y: [IN:GET_WEATHER [SL:ATTRIBUTE rainfall] [SL:DATE today\n",
      "] ]  Here, IN:GET_WEATHER is the high-level intent, SL:ATTRIBUTE and SL:DATE are the slots that spec- ify details about\n",
      "the intent. We refer to the logical-\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #1\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "9 Ethical Considerations  We utilize large language models to translate datasets initially available in English into\n",
      "several languages. The real-world deployment of models trained on LLM-translated data should undergo a careful review of\n",
      "any harmful biases. However, the LLM-translated data and the logical-forms gen- erated by a semantic parser are not\n",
      "user-facing, thus a smaller risk of any direct harms. The in- tended users of any semantic parsing model must be made\n",
      "aware that the answers returned by the model could be incorrect, more so for user-queries in low-resource languages. We\n",
      "do not immediately foresee any serious negative implications of the speciﬁc contributions that we make in this work.  10\n",
      "Acknowledgment  We thank Rahul Goel, Slav Petrov, and Kartikeya Badola for discussions and their feedback on an ear-\n",
      "lier draft of this paper. We thank Massimo Nicosia for sharing the TAF translated datasets and helpful discussions\n",
      "during this project.  References\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #2\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "and  Few-Shot  7 Conclusion  We present a method of utilizing large language models (LLMs) for bootstrapping\n",
      "multilingual se- mantic parsers across several languages. In compar- ison to using off-the-shelf translation services\n",
      "that rely on signiﬁcant amounts of human supervision, we demonstrate that prompting self-supervised LLMs can be a more\n",
      "effective and scalable alter-  native for dataset translation. We ﬁnd that gen- erating multiple diverse translations\n",
      "using sam- pling techniques followed by consistency-based ﬁltering make the translated datasets more effec- tive for\n",
      "training multilingual semantic parsers. On 41 out of 50 typologically diverse languages within two large datasets\n",
      "spanning several domains, our method outperforms a strong translate-train method that utilizes a supervised translation\n",
      "service.  8 Limitations\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #3\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "., 2021; Gritta et al., 2022; Wang et al., 2022). The projection step is often learned independent of the translation\n",
      "service, resulting in poor generalization across languages. In this work we aim to utilize the few-shot gen- eralization\n",
      "abilities exhibited by large language models (LLMs) (Brown et al., 2020; Chowdh- ery et al., 2022; Scao et al., 2022)\n",
      "for bootstrap- ping semantic parsing datasets across ﬁfty lan- guages. We propose a recipe of using LLMs to translate an\n",
      "English semantic parsing dataset containing (utterance, logical-form) pairs: Deng = {(xi eng, yi eng)} into a\n",
      "corresponding dataset in a target language: Dtgt = {(xi tgt, yi tgt)}. The gen- erated dataset Dtgt is then used to\n",
      "train a semantic parser in the target language. Our method uses a small amount of manually translated semantic parsing\n",
      "examples to teach the LLM how to trans- late English examples in the target language via in-context learning (Min et\n",
      "al., 2022).\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #4\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "We consider the problem of automatically syn- thesizing semantic parsing datasets across several ∗Work done during an\n",
      "internship at Google Research  Figure 1 describes our data-translation pipeline which we refer to as LLM-T (§ 3). In\n",
      "contrast  Figure 1: Proposed semantic parsing data translation pipeline using LLMs (§ 3): With the help of human\n",
      "translators, we ﬁrst collect translations of a small seed set of English examples in the Target Language (e.g. Hindi; §\n",
      "3.1). Given a new English example, a small subset from this initial seed set of examples with their respective\n",
      "translations is chosen to prompt the LLM (§ 3.2). The prompted LLM translates the given English example in the Target\n",
      "Language. We repeat this process for each example in the English training data to generate a training dataset in the\n",
      "Target Language. To ensure high-quality of the resulting dataset, we generate diverse translations via top-p (nucleus)\n",
      "sampling (§ 3.3) and apply consistency ﬁltering (§ 3.4).\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #5\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf\n",
      "................................................................................\n",
      "Content: \n",
      ". The translated examples are then used to train a downstream mul- tilingual semantic parser. For example, Nicosia et\n",
      "al. (2021) implement the project step by training a ﬁller module on English data to ﬁll slot-values in a logical-form\n",
      "signature by copying spans from the utterance. During inference, the trained ﬁller module is then used in a zero-shot\n",
      "manner to ﬁll logical-form signatures with spans copied from the translated utterances.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #6\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "[SL:ATTRIBUTE  rainfall]  Semantic Parsing Model We use a pre-trained mT5-Large checkpoint (1.2B parameters) to initial-\n",
      "ize the downstream semantic parsing models that map utterances in the input to logical-forms in the output. We ﬁnetune\n",
      "the mT5 model on the original English dataset mixed with the translated datasets in target languages. We train using the\n",
      "Adafactor optimizer (Shazeer and Stern, 2018) with a ﬁxed learning rate of 1e−3 and a batch size of 256, for 30K steps\n",
      "using the T5X library (Roberts et al., 2022) on 64 TPU-v3 chips. Examples from each language are sampled uniformly for\n",
      "batch creation. For model selection, we choose the best perform- ing checkpoint as per the dev splits and report our\n",
      "results on the test splits.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #7\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "5 Results and Analysis  We ﬁrst present downstream performance of seman- tic parsing models trained on data generated by\n",
      "our method (§ 5.1) and compare with zero-shot setting, few-shot setting, and the TAF method (Nicosia et al., 2021). We\n",
      "then compare our method against the “full-shot” skyline where we utilize the origi- nal training datasets that were\n",
      "manually translated with the help of human annotators in the target languages (§ 5.2). We then present an analysis of\n",
      "different design choices that result in effective data translation using LLM-T (§ 5.3). Finally, we present an error\n",
      "analysis to show the key sources of disagreements between the parser predictions and the ground truth (§ 5.4). All the\n",
      "experiments use our corrected EM metric (§ 4; Evaluation Metric).  5.1 Evaluation on MTOP and MASSIVE\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #8\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "Despite cross-lingual generalization demon- strated by pre-trained multilingual models, the translate-train paradigm of\n",
      "transferring En- glish datasets across multiple languages re- mains to be a key mechanism for training task- speciﬁc\n",
      "multilingual models. However, for many low-resource languages, the availability of a reliable translation service\n",
      "entails signiﬁ- cant amounts of costly human-annotated trans- lation pairs. Further, translation services may continue\n",
      "to be brittle due to domain mismatch between task-speciﬁc input text and general- purpose text used for training\n",
      "translation mod- els. For multilingual semantic parsing, we demonstrate the effectiveness and ﬂexibility offered by\n",
      "large language models (LLMs) for translating English datasets into several lan- guages via few-shot prompting\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #9\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: bootstrapping-multilingual-semantic-parsers-using-large-language-models.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "languages. Semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013) is the task of\n",
      "mapping natural language text into an executable logical-form. For exam- ple, given a user instruction (x) : “Wake me up\n",
      "by 5 am”, mapping it to the logical-form (y): [IN:CREATE_ALARM [SL:DATE_TIME 5 am ]]. Manual annotation of queries with\n",
      "their logical forms requires human expertise which makes data collection across multiple languages challenging. A common\n",
      "approach to automatic multilingual dataset creation is translating existing English datasets into target languages.\n",
      "Prior methods uti- lize an off-the-shelf machine translation model for translating the English utterance into the target\n",
      "lan- guage xeng → xtgt, followed by projecting lan- guage speciﬁc components in the English logical- form yeng to obtain\n",
      "the logical-form ytgt in the tar- get language (Moradshahi et al., 2020, 2021; Xia and Monti, 2021; Nicosia et al.,\n",
      "2021; Gritta et al., 2022; Wang et al\n",
      "................................................................................\n",
      "Response:  For low-resource languages, the availability of reliable translation services often requires significant amounts of\n",
      "costly human-annotated translation pairs. Additionally, translation services may still be brittle due to domain mismatch\n",
      "between task-specific input text and general-purpose text used for training translation models.\n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "ask(\"How does semantic parsing work with low resource languages?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a818a414acbd"
   },
   "source": [
    "Let's ask a question which is outside of the domain in the corpus. You should see something like - \"I cannot determine the answer to that\". This is because the output is conditioned in the prompts to not to respond when the question is out of the context.\n",
    "\n",
    "Following is the instructions in prompt template that is configured in the retrieval QA chain above:\n",
    "\n",
    "```\n",
    "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
    "\n",
    "Do not try to make up an answer:\n",
    " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
    " - If the context is empty, just say \"I do not know the answer to that.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9b68d07090b4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
      "\n",
      "Question: What is NFC?\n",
      "\n",
      "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
      "\n",
      "Do not try to make up an answer:\n",
      " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
      " - If the context is empty, just say \"I do not know the answer to that.\"\n",
      "\n",
      "=============\n",
      "<latexit sha1_base64=\"+t0/fCT+bwdn8UYzdjxaOo9NJME=\">AAACPHicfZDLSsNAFIYn9V6v1aWbYBVFpCQi6rKgC12IF2wrNKWcTE/SoZNJmJmINfQN3Orj+B7u3Ylb105rBW94YODjP//MnPP7CWdKO86TlRsZHRufmJzKT8/Mzs0vFBarKk4lxQqNeSyvfFDImcCKZprjVSIRIp9jze8c9Pu1a5SKxeJSdxNsRBAKFjAK2kgXt+vNhaJTcgZl/wZ3CEUyrLNmwVr1WjFNIxSaclCq7jqJbmQgNaMce3kvVZgA7UCIdYMCIlSNbDBqz14zSssOYmmO0PZA/Xojg0ipbuQbZwS6rX72+uJfvXqqg/1GxkSSahT046Mg5baO7f7edotJpJp3DQCVzMxq0zZIoNqkk/cO0ewi8cS8e5qgBB3LzcwDGUZw0zO7hd5Wn/4zMvFpNJQ3ubo/U/wN1e2Su1vaOd8plo+HCU+SZbJCNohL9kiZHJEzUiGUBOSO3JMH69F6tl6s1w9rzhreWSLfynp7By8brgI=</latexit>z0\n",
      "\n",
      "<latexit sha1_base64=\"mPpLqvfiTr3sYUH3/8vodE3xifA=\">AAACPHicfZDLSgMxFIYzXmu969JNsAoiUmakqMuCLnQhXrC10ClyJj3ThmYyQ5IR69A3cKuP43u4dyduXZvWCt7wQODjP3+Sc/4gEVwb131yRkbHxicmc1P56ZnZufmFxaWqjlPFsMJiEataABoFl1gx3AisJQohCgReBp39fv/yGpXmsbww3QQbEbQkDzkDY6XzW3q1UHCL7qDob/CGUCDDOr1adNb8ZszSCKVhArSue25iGhkow5nAXt5PNSbAOtDCukUJEepGNhi1R9et0qRhrOyRhg7UrzcyiLTuRoF1RmDa+mevL/7Vq6cm3GtkXCapQck+PgpTQU1M+3vTJlfIjOhaAKa4nZWyNihgxqaT9w/Q7qLw2L57kqACE6vNzAfViuCmZ3dr+Vt9+s/I5afRUt7m6v1M8TdUt4veTrF0ViqUj4YJ58gKWSUbxCO7pEwOySmpEEZCckfuyYPz6Dw7L87rh3XEGd5ZJt/KeXsHIjqt+w==</latexit>z\n",
      "\n",
      "<latexit sha1_base64=\"KqzGpleD13x0G7Vrlw5EMNlUH1U=\">AAACRXicfVBNb9NAEF2Xr5JSaOmxlxUpEqoqyzZWEm4V9NALokikLYqjaryZJKusd63dMRBZ+Rm9ws/hN/AjekNcYZOmEkUtT1rp6c2b2ZmXl0o6iqIfwcqdu/fuP1h92Fh7tP74ycbm02NnKiuwK4wy9jQHh0pq7JIkhaelRShyhSf55M28fvIJrZNGf6Bpif0CRloOpQDyUi/7LAc4Bqo/zs42mlGYJmn8ssWjsJO0X7VST5J2q5MkPA6jBZpsiaOzzWAnGxhRFahJKHCuF0cl9WuwJIXCWSOrHJYgJjDCnqcaCnT9erHzjD/3yoAPjfVPE1+of3fUUDg3LXLvLIDG7t/aXLyp1qto2OnXUpcVoRaXHw0rxcnweQB8IC0KUlNPQFjpd+ViDBYE+Zga2QH6Wyy+9XPflWiBjN2tM7CjAr7M/G2jbG/O/meU+sroWcPnehUev50cJ2HcCtP3aXP/9TLhVbbNnrEXLGZtts8O2RHrMsEMO2df2bfge3AR/Ax+XVpXgmXPFruG4PcfZ8Kymg==</latexit>bY\n",
      "\n",
      "follow the FAIR principles?\n",
      "\n",
      "Identifiers. Consider how often you have published data on your own web site or submitted a zip file along with your paper? Such datasets lack identifiers that are either persistent (a URL to your site will change) or dereferenceable\n",
      "\n",
      "(can we always find a dataset by its identifier?). The publishing industry has long since found a solution for referencing\n",
      "\n",
      "artifacts: unique, persistent, dereferenceable identifiers. We can refer to an artifact by a string of characters and numbers\n",
      "\n",
      "that uniquely identify it; there is a permanent URL that will always get redirected to the main page of the artifact, even if that particular page moves somewhere. Digital object identifiers (DOIs), compact identifiers,18 and similar schemes all serve this purpose.\n",
      "\n",
      "<latexit sha1_base64=\"rODIjbgl+1AbQU4DQzfN+yZaqbc=\">AAACQHicfVBNTxsxEPVS2tL0C8qRi9W0UlVVK28aSLghwYEeUEFqAlI2QrPOJLh47ZU9i4hW+Q9c6c/pv+g/4Ia49oQTgtRWbZ9k6enNm/HMywqtPAnxI1p4sPjw0eOlJ7Wnz56/eLm88qrrbekkdqTV1h1l4FErgx1SpPGocAh5pvEwO92e1g/P0HllzRcaF9jPYWTUUEmgIHXTs4Elf7xcF3GrkSTtJhdxIj4mrc1ARHu9IVo8icUMdTbH/vFK9CYdWFnmaEhq8L6XiIL6FThSUuOklpYeC5CnMMJeoAZy9P1qtu6Evw3KgA+tC88Qn6m/dlSQez/Os+DMgU78n7Wp+Ldar6Rhu18pU5SERt59NCw1J8unt/OBcihJjwMB6VTYlcsTcCApJFRLdzDc4nAvzP1coAOy7n2VghvlcD4Jt43SD1P2P6My98bAaiHX+/D4v0m3EScbcfOgWd/6NE94ia2x1+wdS1iLbbFdts86TLKv7IJdsm/R9+gquo5u7qwL0bxnlf2G6Octxh2wVw==</latexit>\n",
      "\n",
      "<latexit sha1_base64=\"eow8BoOVgK6gRUw64Z0JIyLW5ro=\">AAACPnicfZDNSsNAFIUn/ht/q0s3wSqISElE1GVRF27EClaFJpSb6W07dDIJMxOxhL6CW30cX8MXcCduXTqpEdSKFwY+zj0zc+8JE86Udt1na2x8YnJqembWnptfWFxaLq1cqTiVFOs05rG8CUEhZwLrmmmON4lEiEKO12HvOO9f36JULBaXup9gEEFHsDajoHPJrynWXC67FXdYzih4BZRJUbVmydrwWzFNIxSaclCq4bmJDjKQmlGOA9tPFSZAe9DBhkEBEaogGw47cDaN0nLasTRHaGeofr+RQaRUPwqNMwLdVb97ufhXr5Hq9mGQMZGkGgX9/KidckfHTr6502ISqeZ9A0AlM7M6tAsSqDb52P4Jml0knpl3zxOUoGO5nfkgOxHcDcxuHX8np/+MTHwZDdkmV+93iqNwtVvx9it7F3vl6lGR8AxZI+tki3jkgFTJKamROqGkS+7JA3m0nqwX69V6+7SOWcWdVfKjrPcPIkCu9g==</latexit> <latexit\n",
      "\n",
      "<latexit sha1_base64=\"H/zPQBHbEU+inRtdEtB/PMWdxBg=\">AAACRnicfZDPSiNBEMZr4u6q2XX9s0cvzWYFEQkzEtSjuAt6ERWMETIh1HQqSZOenqG7RwxDXmOv+ji+gi/hTbzaEyOoEQsafnz1dXfVF6VSGOv7d15p5svXb7Nz8+XvPxZ+Li4tr5ybJNOc6jyRib6I0JAUiupWWEkXqSaMI0mNaPC36DcuSRuRqDM7TKkVY0+JruBonRSGMdo+R8kO2n57qeJX/XGxaQgmUIFJnbSXvT9hJ+FZTMpyicY0Az+1rRy1FVzSqBxmhlLkA+xR06HCmEwrHw89YmtO6bBuot1Rlo3V1zdyjI0ZxpFzFkOa971C/KjXzGx3t5ULlWaWFH/+qJtJZhNWJMA6QhO3cugAuRZuVsb7qJFbl1M5/EduF01H7t3jlDTaRG/kIepejFcjt1sv3CzoM6NQL0ZHZZdr8D7FaTjfqgbb1dpprbK3P0l4DlbhN6xDADuwB4dwAnXgkMJ/uIYb79a79x68x2dryZvc+QVvqgRPPpyw6w==</latexit>G0<latexit\n",
      "\n",
      "<latexit sha1_base64=\"rtd53eRvdjXL7aJ/XJ2AToV2vfk=\">AAACR3icfVBNbxMxEPWGAm34auHIxWpAQgit7BCacKugBy6IVmraouwqmnUmiVWvd2vPAtEqv4Mr/Tn9CfwKbogjTppKgIAnWXp682Y887LSaE9CfI0a19au37i5vtG8dfvO3XubW/ePfFE5hX1VmMKdZODRaIt90mTwpHQIeWbwODt9vagff0DndWEPaVZimsPE6rFWQEFKk496hFOg+v18KIebLRF321L2OlzEUjyX3ZeBiN6LtuhyGYslWmyF/eFW9CgZFarK0ZIy4P1AipLSGhxpZXDeTCqPJahTmOAgUAs5+rRebj3nj4My4uPChWeJL9VfO2rIvZ/lWXDmQFP/Z20h/q02qGjcS2tty4rQqsuPxpXhVPBFBHykHSoys0BAOR125WoKDhSFoJrJHoZbHL4Nc9+V6IAK97ROwE1y+DQPt02SZwv2P6O2V8bAmiHXq/D4v8lRO5Y7ceeg09p9tUp4nT1k2+wJk6zLdtkbts/6TLEz9pl9YefRRfQt+h79uLQ2olXPA/YbGtFPpeOyLg==</latexit>bY1<latexit\n",
      "\n",
      "<latexit sha1_base64=\"vXrCpQE7bIqHA3pzUtWhaecwsVQ=\">AAACPnicfZDNSsNAFIUn/ht/q0s3wSqISElE1GVRF27EClaFJpSb6W07dDIJMxOxhL6CW30cX8MXcCduXTqpEdSKFwY+zj0zc+8JE86Udt1na2x8YnJqembWnptfWFxaLq1cqTiVFOs05rG8CUEhZwLrmmmON4lEiEKO12HvOO9f36JULBaXup9gEEFHsDajoHPJr3VZc7nsVtxhOaPgFVAmRdWaJWvDb8U0jVBoykGphucmOshAakY5Dmw/VZgA7UEHGwYFRKiCbDjswNk0Sstpx9IcoZ2h+v1GBpFS/Sg0zgh0V/3u5eJfvUaq24dBxkSSahT086N2yh0dO/nmTotJpJr3DQCVzMzq0C5IoNrkY/snaHaReGbePU9Qgo7lduaD7ERwNzC7dfydnP4zMvFlNGSbXL3fKY7C1W7F26/sXeyVq0dFwjNkjayTLeKRA1Ilp6RG6oSSLrknD+TRerJerFfr7dM6ZhV3VsmPst4/AA34rus=</latexit>\u0000\n",
      "\n",
      "For each approximate kernel and also for the true positive-pair kernel K+, we apply Kernel PCA to extract the eigenfunctions ˆfi and their associated eigenvalues ˆλi. We use full-population Kernel PCA for the overlapping regions task, and combine the Nyström method (Williams & Seeger, 2000) with a large random subset of augmented examples to approximate Kernel PCA for the MNIST task. We additionally investigate training a Neural EF model (Deng et al., 2022) to directly decompose the positive pair kernel into principal components using a single model instead of separately training a contrastive learning model, as mentioned in Section 5. We modify the Neural EF objective slightly to make it use positive pair samples instead of kernel evaluations and to increase numerical stability. See Appendix E for additional discussion of the eigenfunction approximation process\n",
      "=============\n",
      "\n",
      "Question: What is NFC?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query: What is NFC?\n",
      "................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #0\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "<latexit\n",
      "sha1_base64=\"+t0/fCT+bwdn8UYzdjxaOo9NJME=\">AAACPHicfZDLSsNAFIYn9V6v1aWbYBVFpCQi6rKgC12IF2wrNKWcTE/SoZNJmJmINfQN3Orj+B7u3Ylb105rBW94YODjP//MnPP7CWdKO86TlRsZHRufmJzKT8/Mzs0vFBarKk4lxQqNeSyvfFDImcCKZprjVSIRIp9jze8c9Pu1a5SKxeJSdxNsRBAKFjAK2kgXt+vNhaJTcgZl/wZ3CEUyrLNmwVr1WjFNIxSaclCq7jqJbmQgNaMce3kvVZgA7UCIdYMCIlSNbDBqz14zSssOYmmO0PZA/Xojg0ipbuQbZwS6rX72+uJfvXqqg/1GxkSSahT046Mg5baO7f7edotJpJp3DQCVzMxq0zZIoNqkk/cO0ewi8cS8e5qgBB3LzcwDGUZw0zO7hd5Wn/4zMvFpNJQ3ubo/U/wN1e2Su1vaOd8plo+HCU+SZbJCNohL9kiZHJEzUiGUBOSO3JMH69F6tl6s1w9rzhreWSLfynp7By8brgI=</latexit>z0\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #1\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "<latexit\n",
      "sha1_base64=\"mPpLqvfiTr3sYUH3/8vodE3xifA=\">AAACPHicfZDLSgMxFIYzXmu969JNsAoiUmakqMuCLnQhXrC10ClyJj3ThmYyQ5IR69A3cKuP43u4dyduXZvWCt7wQODjP3+Sc/4gEVwb131yRkbHxicmc1P56ZnZufmFxaWqjlPFsMJiEataABoFl1gx3AisJQohCgReBp39fv/yGpXmsbww3QQbEbQkDzkDY6XzW3q1UHCL7qDob/CGUCDDOr1adNb8ZszSCKVhArSue25iGhkow5nAXt5PNSbAOtDCukUJEepGNhi1R9et0qRhrOyRhg7UrzcyiLTuRoF1RmDa+mevL/7Vq6cm3GtkXCapQck+PgpTQU1M+3vTJlfIjOhaAKa4nZWyNihgxqaT9w/Q7qLw2L57kqACE6vNzAfViuCmZ3dr+Vt9+s/I5afRUt7m6v1M8TdUt4veTrF0ViqUj4YJ58gKWSUbxCO7pEwOySmpEEZCckfuyYPz6Dw7L87rh3XEGd5ZJt/KeXsHIjqt+w==</latexit>z\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #2\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "<latexit\n",
      "sha1_base64=\"KqzGpleD13x0G7Vrlw5EMNlUH1U=\">AAACRXicfVBNb9NAEF2Xr5JSaOmxlxUpEqoqyzZWEm4V9NALokikLYqjaryZJKusd63dMRBZ+Rm9ws/hN/AjekNcYZOmEkUtT1rp6c2b2ZmXl0o6iqIfwcqdu/fuP1h92Fh7tP74ycbm02NnKiuwK4wy9jQHh0pq7JIkhaelRShyhSf55M28fvIJrZNGf6Bpif0CRloOpQDyUi/7LAc4Bqo/zs42mlGYJmn8ssWjsJO0X7VST5J2q5MkPA6jBZpsiaOzzWAnGxhRFahJKHCuF0cl9WuwJIXCWSOrHJYgJjDCnqcaCnT9erHzjD/3yoAPjfVPE1+of3fUUDg3LXLvLIDG7t/aXLyp1qto2OnXUpcVoRaXHw0rxcnweQB8IC0KUlNPQFjpd+ViDBYE+Zga2QH6Wyy+9XPflWiBjN2tM7CjAr7M/G2jbG/O/meU+sroWcPnehUev50cJ2HcCtP3aXP/9TLhVbbNnrEXLGZtts8O2RHrMsEMO2df2bfge3AR/Ax+XVpXgmXPFruG4PcfZ8Kymg==</latexit>bY\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #3\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: making-computer-science-data-fair.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "follow the FAIR principles?  Identifiers. Consider how often you have published data on your own web site or submitted a\n",
      "zip file along with your paper? Such datasets lack identifiers that are either persistent (a URL to your site will\n",
      "change) or dereferenceable  (can we always find a dataset by its identifier?). The publishing industry has long since\n",
      "found a solution for referencing  artifacts: unique, persistent, dereferenceable identifiers. We can refer to an\n",
      "artifact by a string of characters and numbers  that uniquely identify it; there is a permanent URL that will always get\n",
      "redirected to the main page of the artifact, even if that particular page moves somewhere. Digital object identifiers\n",
      "(DOIs), compact identifiers,18 and similar schemes all serve this purpose.\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #4\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "<latexit\n",
      "sha1_base64=\"rODIjbgl+1AbQU4DQzfN+yZaqbc=\">AAACQHicfVBNTxsxEPVS2tL0C8qRi9W0UlVVK28aSLghwYEeUEFqAlI2QrPOJLh47ZU9i4hW+Q9c6c/pv+g/4Ia49oQTgtRWbZ9k6enNm/HMywqtPAnxI1p4sPjw0eOlJ7Wnz56/eLm88qrrbekkdqTV1h1l4FErgx1SpPGocAh5pvEwO92e1g/P0HllzRcaF9jPYWTUUEmgIHXTs4Elf7xcF3GrkSTtJhdxIj4mrc1ARHu9IVo8icUMdTbH/vFK9CYdWFnmaEhq8L6XiIL6FThSUuOklpYeC5CnMMJeoAZy9P1qtu6Evw3KgA+tC88Qn6m/dlSQez/Os+DMgU78n7Wp+Ldar6Rhu18pU5SERt59NCw1J8unt/OBcihJjwMB6VTYlcsTcCApJFRLdzDc4nAvzP1coAOy7n2VghvlcD4Jt43SD1P2P6My98bAaiHX+/D4v0m3EScbcfOgWd/6NE94ia2x1+wdS1iLbbFdts86TLKv7IJdsm/R9+gquo5u7qwL0bxnlf2G6Octxh2wVw==</latexit>\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #5\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "<latexit\n",
      "sha1_base64=\"eow8BoOVgK6gRUw64Z0JIyLW5ro=\">AAACPnicfZDNSsNAFIUn/ht/q0s3wSqISElE1GVRF27EClaFJpSb6W07dDIJMxOxhL6CW30cX8MXcCduXTqpEdSKFwY+zj0zc+8JE86Udt1na2x8YnJqembWnptfWFxaLq1cqTiVFOs05rG8CUEhZwLrmmmON4lEiEKO12HvOO9f36JULBaXup9gEEFHsDajoHPJrynWXC67FXdYzih4BZRJUbVmydrwWzFNIxSaclCq4bmJDjKQmlGOA9tPFSZAe9DBhkEBEaogGw47cDaN0nLasTRHaGeofr+RQaRUPwqNMwLdVb97ufhXr5Hq9mGQMZGkGgX9/KidckfHTr6502ISqeZ9A0AlM7M6tAsSqDb52P4Jml0knpl3zxOUoGO5nfkgOxHcDcxuHX8np/+MTHwZDdkmV+93iqNwtVvx9it7F3vl6lGR8AxZI+tki3jkgFTJKamROqGkS+7JA3m0nqwX69V6+7SOWcWdVfKjrPcPIkCu9g==</latexit>\n",
      "<latexit\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #6\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "<latexit\n",
      "sha1_base64=\"H/zPQBHbEU+inRtdEtB/PMWdxBg=\">AAACRnicfZDPSiNBEMZr4u6q2XX9s0cvzWYFEQkzEtSjuAt6ERWMETIh1HQqSZOenqG7RwxDXmOv+ji+gi/hTbzaEyOoEQsafnz1dXfVF6VSGOv7d15p5svXb7Nz8+XvPxZ+Li4tr5ybJNOc6jyRib6I0JAUiupWWEkXqSaMI0mNaPC36DcuSRuRqDM7TKkVY0+JruBonRSGMdo+R8kO2n57qeJX/XGxaQgmUIFJnbSXvT9hJ+FZTMpyicY0Az+1rRy1FVzSqBxmhlLkA+xR06HCmEwrHw89YmtO6bBuot1Rlo3V1zdyjI0ZxpFzFkOa971C/KjXzGx3t5ULlWaWFH/+qJtJZhNWJMA6QhO3cugAuRZuVsb7qJFbl1M5/EduF01H7t3jlDTaRG/kIepejFcjt1sv3CzoM6NQL0ZHZZdr8D7FaTjfqgbb1dpprbK3P0l4DlbhN6xDADuwB4dwAnXgkMJ/uIYb79a79x68x2dryZvc+QVvqgRPPpyw6w==</latexit>G0<latexit\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #7\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "<latexit\n",
      "sha1_base64=\"rtd53eRvdjXL7aJ/XJ2AToV2vfk=\">AAACR3icfVBNbxMxEPWGAm34auHIxWpAQgit7BCacKugBy6IVmraouwqmnUmiVWvd2vPAtEqv4Mr/Tn9CfwKbogjTppKgIAnWXp682Y887LSaE9CfI0a19au37i5vtG8dfvO3XubW/ePfFE5hX1VmMKdZODRaIt90mTwpHQIeWbwODt9vagff0DndWEPaVZimsPE6rFWQEFKk496hFOg+v18KIebLRF321L2OlzEUjyX3ZeBiN6LtuhyGYslWmyF/eFW9CgZFarK0ZIy4P1AipLSGhxpZXDeTCqPJahTmOAgUAs5+rRebj3nj4My4uPChWeJL9VfO2rIvZ/lWXDmQFP/Z20h/q02qGjcS2tty4rQqsuPxpXhVPBFBHykHSoys0BAOR125WoKDhSFoJrJHoZbHL4Nc9+V6IAK97ROwE1y+DQPt02SZwv2P6O2V8bAmiHXq/D4v8lRO5Y7ceeg09p9tUp4nT1k2+wJk6zLdtkbts/6TLEz9pl9YefRRfQt+h79uLQ2olXPA/YbGtFPpeOyLg==</latexit>bY1<latexit\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #8\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: a-mixture-of-expert-approach-to-rl-based-dialogue-management.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "<latexit\n",
      "sha1_base64=\"vXrCpQE7bIqHA3pzUtWhaecwsVQ=\">AAACPnicfZDNSsNAFIUn/ht/q0s3wSqISElE1GVRF27EClaFJpSb6W07dDIJMxOxhL6CW30cX8MXcCduXTqpEdSKFwY+zj0zc+8JE86Udt1na2x8YnJqembWnptfWFxaLq1cqTiVFOs05rG8CUEhZwLrmmmON4lEiEKO12HvOO9f36JULBaXup9gEEFHsDajoHPJr3VZc7nsVtxhOaPgFVAmRdWaJWvDb8U0jVBoykGphucmOshAakY5Dmw/VZgA7UEHGwYFRKiCbDjswNk0Sstpx9IcoZ2h+v1GBpFS/Sg0zgh0V/3u5eJfvUaq24dBxkSSahT086N2yh0dO/nmTotJpJr3DQCVzMzq0C5IoNrkY/snaHaReGbePU9Qgo7lduaD7ERwNzC7dfydnP4zMvFlNGSbXL3fKY7C1W7F26/sXeyVq0dFwjNkjayTLeKRA1Ilp6RG6oSSLrknD+TRerJerFfr7dM6ZhV3VsmPst4/AA34rus=</latexit>\u0000\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE #9\n",
      "--------------------------------------------------------------------------------\n",
      "Document Source: genai-423417-documents/google-research-pdfs\n",
      "Document Name: contrastive-learning-can-find-an-optimal-basis-for-approx-view-invariant-functions.pdf\n",
      "................................................................................\n",
      "Content: \n",
      "For each approximate kernel and also for the true positive-pair kernel K+, we apply Kernel PCA to extract the\n",
      "eigenfunctions ˆfi and their associated eigenvalues ˆλi. We use full-population Kernel PCA for the overlapping regions\n",
      "task, and combine the Nyström method (Williams & Seeger, 2000) with a large random subset of augmented examples to\n",
      "approximate Kernel PCA for the MNIST task. We additionally investigate training a Neural EF model (Deng et al., 2022) to\n",
      "directly decompose the positive pair kernel into principal components using a single model instead of separately\n",
      "training a contrastive learning model, as mentioned in Section 5. We modify the Neural EF objective slightly to make it\n",
      "use positive pair samples instead of kernel evaluations and to increase numerical stability. See Appendix E for\n",
      "additional discussion of the eigenfunction approximation process\n",
      "................................................................................\n",
      "Response:  The provided context does not mention anything about NFC, so I cannot answer this question.\n",
      "................................................................................\n"
     ]
    }
   ],
   "source": [
    "ask(\"What is NFC?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a857c315e07c"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1560e9a4-0322-4344-8375-9ac7b326713c"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
